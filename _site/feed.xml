<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Language Documentation</title>
    <description></description>
    <link>http://langdoc.github.io/</link>
    <atom:link href="http://langdoc.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 25 Mar 2016 22:21:53 +0100</pubDate>
    <lastBuildDate>Fri, 25 Mar 2016 22:21:53 +0100</lastBuildDate>
    <generator>Jekyll v3.0.0</generator>
    
      <item>
        <title>Isogloss maps with Shapefiles</title>
        <description>&lt;p&gt;I’ve been working lately a lot with language areas as different Shapefiles. This works nicely, and usually it is quite unproblematic to make a functional map. There are some problems I haven’t solved, mainly with the best way to store different attributes relevant for languages, but which I wouldn’t like to store repeatingly inside the Shapefile. But now I tackle a bit different issue, which is the use of Shapefiles to mark isoglosses. As usually, I don’t want to take the easiest route, but would look for something efficient, economic, pretty and easy to manage. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Let’s take a look into a Shapefile, or Geojson, whatever. When the data is read to R as &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt; it behaves the same way whatever the origin. It is bit more complex format which has slots for different kinds of information. The most important ones are &lt;code&gt;@data&lt;/code&gt; and &lt;code&gt;@polygons&lt;/code&gt;. The first is just a data frame, and it can be accessed with &lt;code&gt;object@data$column&lt;/code&gt;. It is possible to do there quite many of those things which you would do with any other data frame, which makes it very powerful tool.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/webmap/komi_basic.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Now there are of course lots of different isoglosses, but they don’t necessarily align by dialects. So we cannot just store information with what kind of isogloss value which polygon is associated. In a way where this line goes can be rather arbitrary, although it is also absolute in the sense that at least on more abstract level we can say this line goes along the line. It is always another question how the isoglosses pattern in our actual corpus data. But I think both representations have their own value and use.&lt;/p&gt;

&lt;p&gt;This is a good example which cuts across all main variants, showing the variation in stress within Komi. It is taken from Batalova’s 1982 book &lt;strong&gt;Ареальные исследования по восточным финно-угорским языкам (коми языки)&lt;/strong&gt;, page 41.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/figures/batalova1982a-41.png&quot; alt=&quot;stress in Komi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To explain it simply, the majority of Komi-Zyrian has no morphologically significant stress, but in some southern dialects this feature is present, as it is in almost all Komi-Permyak, besides small eastern region where the stress is connected to vowel system more widely, as it is in all Komi-Yazva. In this case almost all action is in the southern varieties, so I’ll try to display it so that the attention is more theere (and some southern varieties are geographically very small, which makes them difficult to show on large scale maps).&lt;/p&gt;

&lt;p&gt;We could take the polygons above and modify a variant which is cut according to this pattern, but this has some problems as well. Mainly that the changes in original polygons would not be reflected in the Shapefiles which contain the isoglosses! This would have fast quite disastrous results, and generally speaking would not be very elegant.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maptools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;isogloss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readShapePoly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Volumes/langdoc/maps/language_maps/isoglosses/kom.shp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proj4string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CRS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;addPolygons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isogloss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;fillOpacity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;gray&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;popup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/webmap/komi_isogloss_blobs.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This is maybe a bit strange approach. But in many ways it is the best I’ve come up with. We can use this command from GDAL to clip the isogloss polygon with the areas defined in the language polygon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ogr2ogr -clipsrc langs/kom/kom.shp isoglosses/kom_isogloss_1_cut.shp isoglosses/kom_isogloss.shp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives us something we can easily use elsewhere on maps. It will look as good as the language polygons do, but we can associate any kind of overlays with it and still be rather sure things are not breaking apart. The isogloss shapefiles will not need special maintenance work when the language polygons are modified, and this is something really important if the idea is to have something more reliable than just graphics used once and forgotten.&lt;/p&gt;

&lt;p&gt;It can be bit annoying ot modify Shapefiles like these, so take a look into snapping options in QGIS to make this bit easier. Otherwise it would be next to impossible to get the polygons align prettily, and I think it is quite crucial there are no holes between the polygons.&lt;/p&gt;

&lt;p&gt;There is nothing really new going on in the map below, but note that we are using &lt;code&gt;RColorBrewer&lt;/code&gt; package to set up the color scheme. The colors are always little bit complicated. On one hand they should just appear automatically and be nice, but that doesn’t work so well with varying number of features, and at times some combinations just don’t work and you want to touch them manually. However, something with &lt;code&gt;RColorBrewer&lt;/code&gt; should be simple enough to have inside a function which tries to select suitable color palette for most of the cases. And if there would be tens of different values to map, it probably would not be always that good idea to place them all on one map to start with.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RColorBrewer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;isogloss_cut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readShapePoly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Volumes/langdoc/maps/language_maps/isoglosses/kom_isogloss_1_cut.shp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proj4string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CRS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as.character&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distinct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isogloss_cut&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RColorBrewer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;brewer.pal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Dark2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;leaflet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;54&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zoom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;addProviderTiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Esri.WorldTopoMap&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;addPolygons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isogloss_cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;fillOpacity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;fillColor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;popup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;stroke&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;addLegend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;bottomright&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Stress in Komi Dialects&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/webmap/komi_isogloss_stress.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;I think the next topic would be how to associate points with polygons, for example, to gather data from polygon layers to the token level data frames where there are different coordinates. It could be quite interesting to locate all speakers whose bithplace and place of residence are on different isogloss areas, as just looking to the adaptation of different variables could be enlightening.&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Mar 2016 18:00:00 +0100</pubDate>
        <link>http://langdoc.github.io/2016/03/23/isogloss-maps.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2016/03/23/isogloss-maps.html</guid>
        
        <category>R</category>
        
        <category>maps</category>
        
        
      </item>
    
      <item>
        <title>Adding shadows to images</title>
        <description>&lt;p&gt;I’m not certain if this is something anyone needs often, but at least I was surprised to learn this can also be done from the command line. So I needed to overlay something on a map as a marker, and already had fired up Gimp and started to edit a shadow file to be used in JavaScript. Little I knew that ImageMagick was able to do that as well!  &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;So if one would start with a file like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/29/Icon256x256.png&quot; alt=&quot;example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With command like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;convert Icon256x256.png &#39;(&#39; +clone -background black -shadow 80x100+100+100 &#39;)&#39; +swap -background none -layers merge +repage cat_shadowed.png
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We get a one with background shadow!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/figures/cat_shadowed.png&quot; alt=&quot;shadow_example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You have to play with the numbers to get the direction, size and distance of the shadow right. I think the main potential use for this is in map icons.&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Mar 2016 10:56:00 +0100</pubDate>
        <link>http://langdoc.github.io/2016/03/23/shadows-to-images.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2016/03/23/shadows-to-images.html</guid>
        
        <category>Terminal</category>
        
        <category>maps</category>
        
        
      </item>
    
      <item>
        <title>Generating sine-waves with R</title>
        <description>&lt;p&gt;This topic may not be immediately relevant, but can still be interesting for somebody. So the question is: How to generate sound files with R, which just play sine wave at one specific frequency. One immediate reason is to test the frequencies you are yourself able to hear. I’m now lost somewhere after 17250 hz, which sounds horrendously low, but maybe should not be so unusual for someone in my age (soon 29). &lt;!--more--&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# I load the packages explicitly here, and also refer to them 
# with their namespaces in individual functions.
# I understand this is somewhat redundant, but I hope it makes 
# it more obvious which function is from which package.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tuneR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seewave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plyr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# I needed to do this on Mac to get it work.
# But please install Sox anyway, it is useful!
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tuneR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setWavPlayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sox&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# These are the frequencies we want
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;12000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;14000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15750&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;16000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;16250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;16500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;16750&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;17000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;17250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;17500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;17750&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;18000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# This is a tiny function that in principle could also just be inside llply itself.
# I find it easier to read this way when it is on its own and named.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;write_sines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;seewave&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savewav&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tuneR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paste0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;~/sounds/sound_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_hz.wav&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# llply just repeats the function write_sines to all elements in rates character vector
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plyr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;llply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write_sines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So basically this just fills the folder &lt;code&gt;sounds&lt;/code&gt; with the files like &lt;code&gt;sound_100_hz.wav&lt;/code&gt; and so on, as specified in the &lt;code&gt;rates&lt;/code&gt; vector. Just modify the values and overwrite the old files. In principle one of course could had generated those numbers more automatically, but I think in this case some manual adjustment is good to be there, as you never know exactly which ones you or your ears need.&lt;/p&gt;

&lt;p&gt;Possible applications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have fun with your cats and high sounds&lt;/li&gt;
  &lt;li&gt;Your recording device usually would capture something up to 24 khz. This leaves us the range of 20-24 khz which is not really used as we humans can’t hear it. Is there anything on these higher frequencies one could do something with?&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 01 Nov 2015 14:56:00 +0100</pubDate>
        <link>http://langdoc.github.io/2015/11/01/sine-waves-with-r.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/11/01/sine-waves-with-r.html</guid>
        
        <category>R</category>
        
        <category>data</category>
        
        
      </item>
    
      <item>
        <title>Data workflow for linguistic fieldwork</title>
        <description>&lt;p&gt;I think quite a lot has been written about the data management workflows in documentary linguistics. Lots of this is about the whole workflow from field to the archive. I want to document here the workflow I’ve personally found very good for the immediate work on the field, and also desire to use later. This doesn’t even touch the issues of annotating, archiving, publishing or disseminating. &lt;strong&gt;I want to stress that this all is something that in my opinion should be done already at the field. Period. Don’t delay it, the consequences are heavy.&lt;/strong&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;I’ve encountered occasionally an idea that taking part into a workflow such as presented here would be a sort of a choice. However, you are already there &lt;strong&gt;if&lt;/strong&gt; you decide to involve both video and audio into your working practices. The moment you decide to bring a videocamera on the field dictates some aspects from your further workflow as well, as it has to take into account the existence of both audio and video data streams. Naturally it is possible to ignore the video, but in that case one could also ask what was it worth to do it in the first place.&lt;/p&gt;

&lt;p&gt;As if the situation would be just about the video. Several modern audio recorders, such as Edirol R-26, automatically produce as a number of audio files from different microphones. In order to work with this data in the most effective way we must somehow take this into account, as an example, by remixing the audio in way that brings up the best aspects of these distinct audio tracks, and recording in such a manner that we actually get the benefit that is there.&lt;/p&gt;

&lt;p&gt;The days of having just one track of audio are over. Are we winning or losing with these changes, that is to be seen. But not taking them into account is an ignorance we cannot afford.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I assume that each part in the presented workflow will change in some point. The technology changes always, and there is no reason to stick into old if some better solution is around. This said, we should not end up using any solutions which we are not thoroughly familiar with. At the same time, it remains the fact that many of our tools are used the most while we are doing fieldwork, which also means that we learn very much about them on the field.&lt;/p&gt;

&lt;h2 id=&quot;the-raw-data&quot;&gt;The raw data&lt;/h2&gt;

&lt;p&gt;As mentioned, the session raw data is getting wilder and wilder. Often we use several microphones, i.e. some lapel mics + one in a boompole to capture the overall sound. Also the camera makes its own audio track, which may be very usable in many cases. With many cameras this audio is also 6 channel surround, with which we have to consider are all those tracks containing what we want in the best possible constellation. Another thing to note is that many video cameras store their audio in Dolby Digital format which is not particularly transparent. Similarly, the video may be captured in AVCHD format, which also contains it’s own particularities.&lt;/p&gt;

&lt;p&gt;Generally speaking in documentary linguistics it has been getting more and more common to capture also video. This also turns it into something one is expected to produce. In principle using two cameras is not out of the question, as it solves many problems there inherently are with the camera angle. One can be used to capture the general overview of the setting, and this gives to the other one more freedom to move around and get experimental. This is where the human component comes in. When we start to zoom camera into something that could be interesting, there is always the danger that &lt;strong&gt;the interesting&lt;/strong&gt; takes place somewhere else. If we use only one camera we have to be careful, but with two there is already more freedom.&lt;/p&gt;

&lt;p&gt;So a normal recording session contains very often at least the several files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;few audio files&lt;/li&gt;
  &lt;li&gt;few video files&lt;/li&gt;
  &lt;li&gt;scan from fieldnotes&lt;/li&gt;
  &lt;li&gt;metadata files&lt;/li&gt;
  &lt;li&gt;photographs of the informants&lt;/li&gt;
  &lt;li&gt;general photograph or screenshot from the recording setting itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In practise the metadata can often be in a database and not in the actual folder with the session raw data, but this is not really the point here.&lt;/p&gt;

&lt;p&gt;However, there are many steps before we actually can start to work with this data.&lt;/p&gt;

&lt;h2 id=&quot;data-processing&quot;&gt;Data processing&lt;/h2&gt;

&lt;p&gt;There are few steps one has to take with the raw data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;synchronization&lt;/li&gt;
  &lt;li&gt;mixing&lt;/li&gt;
  &lt;li&gt;cutting&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is sort of a post-production phase which has not existed traditionally, but nowadays is there all the time. I’ve been using &lt;strong&gt;PluralEyes&lt;/strong&gt; and &lt;strong&gt;Final Cut Pro X&lt;/strong&gt; to do this. There are certainly other alternatives, but as far as I know there aren’t really open source solutions for this.&lt;/p&gt;

&lt;h3 id=&quot;pluraleyes&quot;&gt;PluralEyes&lt;/h3&gt;

&lt;p&gt;What PluralEyes does is to take all the audio and video files and to look for matching segments in the audio spectrograms. This must be very similar to audio fingerprinting tools, used i.e. with Python library DejaVu. However, good side of PluralEyes is that it is easy and fast. It also exports the synchronization metadata as an XML file.&lt;/p&gt;

&lt;h3 id=&quot;final-cut-pro&quot;&gt;Final Cut Pro&lt;/h3&gt;

&lt;p&gt;Final Cut Pro is a commercial video editing software, used widely by different video professionals and amateurs alike. I guess Adobe Premiere does more or less the same job equally well. Also Final Cut Pro allows us to import and export session data as an XML file, by the way.&lt;/p&gt;

&lt;p&gt;In PluralEyes we don’t really do anything for the files, but in Final Cut Pro we can start to play more with them. Naturally, the question of cutting is very acute. Are we supposed to cut? What kind of videos are we doing as documentary linguists? Well, I guess the answer is that we are doing different videos for different uses. As an example, for a DVD that is sent to informant I may do different cuts than I do for the one that will be transcribed.&lt;/p&gt;

&lt;p&gt;At least we have good rationale to cut something away from the beginning and the end, as this often has some noise from setting up the tools and preparing for the session. The main reason to cut, in my opinion, is that often this part is captured by one of the devices, i.e. there is an audio track, but the video camera is just being set up. So it just makes it nicer if we leave that hassle out. On the other hand, this often contains very critical information about how the aims and goals of the session are explained to the informant, how the linguists negotiated among each others who stays where, how the room was rearranged to make recording work better etc. These things do influence the recording quite a lot, so it is very good if that part of the work is also documented. However, should that be part of the transcribed session? Maybe. Should that be sent to the informant who maybe just wants to have a nice DVD with the actual interview?&lt;/p&gt;

&lt;p&gt;Another example is that often the recording is interrupted and the technology is turned off for some time. As an example, we may turn camera and recorder off during a phone call or if someone has to leave for some personal errands. However, it is very important question can we just glue these two pieces together, or should we split these two into two different sessions? I’ve always split them into two, since I think the idea of a session is to represent one temporally consecutive situation. However, if the breaks are somehow very well indicated, I believe there is very little reason not to play around with these things if there is the need. One need would be to make an actually nice video from shorter video and audio recordings which are somehow topically interrelated. This is also nicer to transcribe and watch than treating each little file as their own session.&lt;/p&gt;

&lt;p&gt;Of course the Final Cut Pro XML files contain data about the ways how files are rearranged and cut.&lt;/p&gt;

&lt;h3 id=&quot;note-about-xml-files&quot;&gt;Note about XML files&lt;/h3&gt;

&lt;p&gt;Naturally it is one thing to archive the XML files produced by these programs and actually do something useful with them. Understanding their structure and actually getting information out from them is not always trivial either. There are no real applications at the moment that would really use this data, similarly, I don’t think it has been very common practise to archive XML files like these. However, I think the only way we can excuse the use of commercial software is to make certain that the data is not locked into these systems. In any points. Certainly, the most comfortable way to work with fcpxml files is to open them in Final Cut Pro. But we have to make sure that is not the only option.&lt;/p&gt;

&lt;h2 id=&quot;audio-mixing&quot;&gt;Audio mixing&lt;/h2&gt;

&lt;p&gt;I must admit that I have no education about this topic. Maybe I should, as I’m a linguist who works with this stuff all the time, but I think there are very few instances of linguistic departments which include audio engineering as a part of their curricula.&lt;/p&gt;

&lt;p&gt;I sort of thought I had got it already work nicely, but then one musician friend of mine was listening some ELAN sessions and complained about an echo. Yes, indeed, several audio tracks played simultaneously make sort of a nice sound where everyone can be heard well, but there was indeed some small echo. I remixed the tracks differently, putting some more quiet, some louder, and the echo disappeared. Not that I would understand anything about the process, but clearly this is an enormous field someone should understand about.&lt;/p&gt;

&lt;p&gt;The situation is even more acute when it comes to work with old recordings. The sound quality is often quite bad, and there are certainly things we can do. I’ve used often ReaFir effect in REAPER to mute some frequencies, though there has been no way to include any audio editing software into the current workflow I have with documentary linguistic sessions. As our modern recordings are almost always very decent there is maybe little space for this, but still the question remains that mixing these audio channels from different devices is not trivial task at all.&lt;/p&gt;

&lt;p&gt;This also means that it has to be very well indicated which are the actual raw data files and which are mixed for actual session. Naturally for phonetic analysis one should consult the raw data, but what you get into ELAN is usually a mix of several audio files, I see no other alternative.&lt;/p&gt;

&lt;h2 id=&quot;processed-data&quot;&gt;Processed data&lt;/h2&gt;

&lt;p&gt;So now the session folder actually contains already these files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;few audio files&lt;/li&gt;
  &lt;li&gt;few video files&lt;/li&gt;
  &lt;li&gt;scan from fieldnotes&lt;/li&gt;
  &lt;li&gt;metadata files&lt;/li&gt;
  &lt;li&gt;photograhps of the informants&lt;/li&gt;
  &lt;li&gt;general photograph or screenshot about the recording setting itself&lt;/li&gt;
  &lt;li&gt;PluralEyes XML&lt;/li&gt;
  &lt;li&gt;Final Cut Pro XML&lt;/li&gt;
  &lt;li&gt;audio file for ELAN&lt;/li&gt;
  &lt;li&gt;video file for ELAN&lt;/li&gt;
  &lt;li&gt;ELAN file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What I have been doing now is to create new folders for different outputs, i.e. I have folder called session_name-i for the file version that has been sent for the informant, that often contains just a DVD disc image. Then there are folders marked with a, b, and c etc. for distinct subsessions if they exist. Usually they do. One problem is that the session can be understood to work out so that if the beginning hassle counts as something of its own, then that’s the part a. Then the actual recording, if uninterrupted, gets the symbol b, and the end hassle is c. So then there is a very large amount of sessions with the symbol b in their name, which feels bit stupid at times, but doesn’t really matter so much. Maybe there are better ways to arrange these files.&lt;/p&gt;

&lt;h2 id=&quot;whats-the-point&quot;&gt;What’s the point?&lt;/h2&gt;

&lt;p&gt;My idea about this workflow is such, that this would be ready for every session when we leave the fieldsite. If we have several video files, this version can be done with the most simple and general video there is. Don’t worry about nice video for the informants, that can be polished and sent back later. Quality wins over speed in that. For the later use it is possible to remix the video with different angles, this doesn’t influence the ELAN file of course in any way, unless there is actual mixing of the content, problems of which I already discussed briefly.&lt;/p&gt;

&lt;p&gt;I already keep track from all metadata on the field, and make sure that everything is arranged into their own folders and metadata is filled up, if possible, at the same evening. More days pass more difficult it gets to actually reconstruct what has happened. I think actually taking it as far as even exporting the media for ELAN files and creating those dummy ELAN files does make sense. If this video management workflow is not taken care of instantly, it will become an enormous burden for someone (I’ve been that someone for several times). Running through this workflow is not very slow in the end, the only slow part is exporting. So process the session data at the evening with a beer or the local mildly alcoholic beverage of choice, leave the computer export and backup over night, repeat. If you have a research assistant or someone else doing this, make sure they are unstressed and have the beverage they deserve, this is not easy work, but it is worth doing on the spot.&lt;/p&gt;

&lt;p&gt;I understand this workflow probably doesn’t work in locations where electricity is scarce. I work mainly in the Northern Russia, and there the villages are often very good locations for using computer at the evenings. I understand this is not always possible also time-wise, but my advice would be not to let this kind of work accumulate. I’ve spent months working with video and audio files from few weeks long fieldtrip, and very big part of that could had been ready even before I got back home.&lt;/p&gt;

&lt;h2 id=&quot;is-anything-going-to-change&quot;&gt;Is anything going to change?&lt;/h2&gt;

&lt;p&gt;Maybe when the computers get faster also video exporting gets faster, though this is unlikely as the video probably gets larger at the same rate. Maybe in some point it gets convenient enough to make notes straight away in digital format. This would change a lot, if one could, as an example, timestamp the notes and associate them with the raw data.&lt;/p&gt;

&lt;p&gt;There has been this development that there is more and more fieldwork equipment. Two cameras means two tripods, and so on. However, there are some interesting developments. As an example, there have been lately more and more camera models which have a high quality objective in very small body. Naturally, one could assume that cameras like these are very good in video, as modern DSLR’s tend to be, and the small size makes them much more user-friendly. Now there are lots of hipstery retro models out there, but in some point we probably start to see products that could be immediately useful for documentary linguists as well. Just a note, in this workflow the video needs only some audio, it can be really bad even, just enough that it can be synchronized with the audio files.&lt;/p&gt;

&lt;p&gt;The majority of events we record tend to be interviews of some sort. They are in many ways near conversations, but there is often the setting that one tells and another asks. Even when there are multiple participants it is normal that someone takes a very central role and does the most of the speaking. It isn’t fully solved how to record more naturally occurring speech events, usually luck has something to do with that. I think one change in the thinking would be to realize that now we basically record people when they are supposed to say something. Instead we maybe should record people when they do something, and if we are lucky, they say something. This means that we would end up recording much more situations where no one actually says a thing. I’m not currently certain how to move forward with that, but I’m sure the coming years will be interesting.&lt;/p&gt;

&lt;p&gt;Of course it is very interesting if we would have recordings that last many hours and contain only few utterances. Why not. Hard disk space is cheap. These languages being used naturally is priceless.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 Sep 2015 12:45:13 +0200</pubDate>
        <link>http://langdoc.github.io/2015/09/16/session-workflow.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/09/16/session-workflow.html</guid>
        
        <category>fieldwork</category>
        
        <category>data</category>
        
        
      </item>
    
      <item>
        <title>Editing text blocks in Emacs</title>
        <description>&lt;p&gt;Often when we digitalize some text collections we end up with text blocks in one language, and the translations on some other language in similar blocks. However, in our modern language collections the text is divided into utterances, which often more or less do match with sentences.&lt;/p&gt;

&lt;p&gt;There is no automatic way to match the sentences, because there isn’t always exact correspondence at the sentence level. However, some ways to work with this data are more difficult than the others. &lt;!--more--&gt; One can start with some regular expressions such as these.&lt;/p&gt;

&lt;p&gt;The point of this is to mark paragraph breaks with the hash character.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;\n&quot; &amp;gt; &quot;\n#\n&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can try to separate the sentences into their own lines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;(\.|\,|\?|\!|\:) &quot; &amp;gt; &quot;$1\n&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works quite well. But there is still mismatch, almost always, at least when we have a longer text. I do this often in TextMate, but any text editor can handle this simple things.&lt;/p&gt;

&lt;h2 id=&quot;editing-with-emacs&quot;&gt;Editing with Emacs&lt;/h2&gt;

&lt;p&gt;I don’t use Emacs that often, though there are some situations where it is irreplaceable. One situation is this. We can open the two text blocks in two diagonally split buffers, and make our editing way easier with following commands in both buffers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;M-x toggle-truncate-lines
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now each line contains one string we want to match. Then let’s have line numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;M-x linum-mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then the last one, which actually makes all this effort worthwhile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;M-x scroll-all-mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the both buffers are scrolling down and up simultaneously. With &lt;code&gt;C-x-o&lt;/code&gt; we can move between the buffers, and by moving sideways between the lines it is easy to keep two trackers matching.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Sep 2015 13:06:13 +0200</pubDate>
        <link>http://langdoc.github.io/2015/09/01/emacs-editing.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/09/01/emacs-editing.html</guid>
        
        <category>emacs</category>
        
        <category>digitalization</category>
        
        
      </item>
    
      <item>
        <title>Fieldwork map</title>
        <description>&lt;p&gt;I started to write some long rant about the data situation with semi-large language like Komi, and ended up combining it to geographical locations, with which I ended into mapping. For quite a while I had problems in embedding Leaflet/R based maps into Github Pages, although now when I realized how it works it is actually ridiculously easy. There are many frameworks to work with maps, but I think at the moment Leaflet is the one doing best. What kind of complicates the things is how one can use Leaflet within different frameworks, basically with JavaScript and with R. I have been working with both of them intensively, and started also collecting different language maps into a new Github repository called &lt;a href=&quot;http://github.com/nikopartanen/language_maps/&quot;&gt;Language Maps&lt;/a&gt;.&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;At the moment the map shows only Komi-Zyrian dialects. I’m on my way to connect it to our database and get some markers from there.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/kpv_dial.html&quot;&gt;&lt;i&gt;just testing here&lt;/i&gt;&lt;/iframe&gt;

&lt;p&gt;Also more complicated work is possible, as an example, the map below shows the languages which are part of the INEL project I’m now working with.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/language_maps_inel_osm.html&quot;&gt;&lt;i&gt;just testing here&lt;/i&gt;&lt;/iframe&gt;

</description>
        <pubDate>Sun, 26 Jul 2015 01:55:14 +0200</pubDate>
        <link>http://langdoc.github.io/2015/07/26/fieldwork-map.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/07/26/fieldwork-map.html</guid>
        
        <category>fieldwork</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>Basic language maps</title>
        <description>&lt;p&gt;I started to write some long rant about the data situation with semi-large language like Komi, and ended up combining it to geographical locations, with which I ended into mapping. For quite a while I had problems in embedding Leaflet/R based maps into Github Pages, although now when I realized how it works it is actually ridiculously easy. There are many frameworks to work with maps, but I think at the moment Leaflet is the one doing best. What kind of complicates the things is how one can use Leaflet within different frameworks, basically with JavaScript and with R. I have been working with both of them intensively, and started also collecting different language maps into a new Github repository called &lt;a href=&quot;http://github.com/nikopartanen/language_maps/&quot;&gt;Language Maps&lt;/a&gt;.&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;I have this idea that we are not using geographical data as complexly as could.&lt;/p&gt;

&lt;p&gt;At the moment the map shows only Komi-Zyrian dialects. I’m on my way to connect it to our database and get some markers from there.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/webmap/kpv_dial.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Also more complicated work is possible, as an example, the map below shows the languages which are part of the INEL project I’m now working with.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;500&quot; src=&quot;http://nikopartanen.github.io/language_maps/webmap/language_maps_inel_osm.html&quot;&gt;&lt;/iframe&gt;

</description>
        <pubDate>Sun, 26 Jul 2015 01:55:14 +0200</pubDate>
        <link>http://langdoc.github.io/2015/07/26/basic-language-maps.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/07/26/basic-language-maps.html</guid>
        
        <category>maps</category>
        
        <category>R</category>
        
        <category>leaflet</category>
        
        
      </item>
    
      <item>
        <title>Google Docs and Markdown</title>
        <description>&lt;p&gt;We were recently writing a small abstract together with a few colleagues (Michael Rießler and Hanna Thiele). We started in Dropbox, but that is, as usually, quite bad in intensive writing. In principle we could use other tools we use already anyway, GitHub or SVN, but the same problem remains that it would be very convenient to see what others are editing and use some clear system for commenting.&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;I personally like to write this kind of simple things in Markdown. It has several advantages over LaTeX. It is easy, fast to type and allows good connection with bibtex bibliography. So I started to think whether there is some way to combine the best of both worlds. I know there are some other collaborative editors for Markdown itself, but there were some other reasons to stick to Google Docs. I have one colleague with who I collaborate regularly, who uses ChromeBook as his main computer. Thereby for him it is the best to stick into that. And this means that for me it is the best to figure out some ways to use Google Docs more effectively.&lt;/p&gt;

&lt;p&gt;The workflow presented here was inspired by this &lt;a href=&quot;http://ericpgreen.com/reproducible-research-with-word/&quot;&gt;blogpost&lt;/a&gt; by Eric P. Green, though it is not at all that sophisticated.&lt;/p&gt;

&lt;p&gt;All code is stored in a GitHub &lt;a href=&quot;www.github.com/langdoc/GoogleDocs_with_md&quot;&gt;repository&lt;/a&gt;, but I go it through here piece by piece. Here are the steps that come.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a YAML header&lt;/li&gt;
  &lt;li&gt;Download text from Google Docs&lt;/li&gt;
  &lt;li&gt;Remove mysterious NUL character from the beginning of file&lt;/li&gt;
  &lt;li&gt;Delete the comments that may be present&lt;/li&gt;
  &lt;li&gt;Combine the header and the text&lt;/li&gt;
  &lt;li&gt;Write those into a markdown document&lt;/li&gt;
  &lt;li&gt;Render HTML&lt;/li&gt;
  &lt;li&gt;Render PDF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is really very simple. &lt;strong&gt;The idea is to read data directly from Google Docs.&lt;/strong&gt; And it allows &lt;a href=&quot;https://docs.google.com/document/d/1D1C_79nLDM25r96dWinufVUd70-ipsHzv6cK-9cK-Io/edit?usp=sharing&quot;&gt;this&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/figures/example_gdocs.png&quot; alt=&quot;Google Docs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To be turned into something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/figures/example_pdf.png&quot; alt=&quot;Example PDF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Or into an HTML page, or into .docx file, or .epub, or basically into anything &lt;a href=&quot;http://pandoc.org/&quot;&gt;Pandoc&lt;/a&gt; supports. So let’s go through step by step how this works.&lt;/p&gt;

&lt;h3 id=&quot;the-header&quot;&gt;The header&lt;/h3&gt;

&lt;p&gt;Header is a simple block of YAML. We are telling it the date, author, title, the location of a bibliography and other such details. It depends a bit from our desired output format what we want to have here, as we can also pass many of these settings directly into Pandoc. Here we just save the header as a character string.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;---
title: &quot;Collaborative authoring in Google Docs&quot;
author: &quot;Niko Partanen&quot;
date: &quot;19 Jun 2015&quot;
pdf_document:
        keep_tex: yes
bibliography: ./bibtex/FRibliography_example.bib
---
&#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;reading-the-file&quot;&gt;Reading the file&lt;/h3&gt;

&lt;p&gt;Then we read the file from Google Docs and write it as a file. The link to Google Docs is very easy to generate. You can get the document ID from the link you use to share it.&lt;/p&gt;

&lt;p&gt;The structure is like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://docs.google.com/document/d/put-id-here/export?format=txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One could have there also some other format, but a simple text file is the best for now.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://docs.google.com/document/d/1D1C_79nLDM25r96dWinufVUd70-ipsHzv6cK-9cK-Io/export?format=txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;temp.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writeLines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;removing-the-nul-character&quot;&gt;Removing the NUL character&lt;/h3&gt;

&lt;p&gt;For some reason the text file contains a &lt;a href=&quot;https://en.wikipedia.org/wiki/Null_character&quot;&gt;NUL character&lt;/a&gt; in the beginning of the first line. I don’t understand at all what it is doing there, but R, LaTeX and Pandoc are all giving errors for it later, so having it around is unacceptable. I was trying to find out how to delete it for quite some time, and ended up to do it from Terminal, by simply removing those first bytes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cat temp.txt | tail -c +4 &amp;gt; temp_clean.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;temp_clean.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I would be very happy to know if there is some easier way to do this!&lt;/p&gt;

&lt;h3 id=&quot;removing-the-comments&quot;&gt;Removing the comments&lt;/h3&gt;

&lt;p&gt;It is possible that there are some comments left into the document. They have formatting like [a] or [b] in the text, and the actual comment text comes to the end. I assume there could be some way to turn them into footnotes in the PDF, but at the moment I just want to delete them.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;(.+## References).+&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;\\1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\\[.\\]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;putting-it-together&quot;&gt;Putting it together&lt;/h3&gt;

&lt;p&gt;Then we simply paste together the header and the document. This is then written into a new Markdown document.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;full_paper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paste0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;example.md&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writeLines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileConn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;converting-the-files&quot;&gt;Converting the files&lt;/h3&gt;

&lt;p&gt;After this we can start to convert the file. Now I convert it into HTML and PDF. &lt;strong&gt;Note for linguists:&lt;/strong&gt; The default fonts used here are not very good for any exotic script. There are lots of situations where the basic fonts in principle can handle what you have there, but in some specific situations (such as within a code block) the font in that environment doesn’t have what you need. Well, this is how the things always are with typography and lesser used languages. But remember to play with Pandoc font settings if you have any non-latin stuff there.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmarkdown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;example.md&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;html_document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pandoc example.md --latex-engine=xelatex --biblio ./bibtex/FRibliography_example.bib -o example.pdf --variable mainfont=Georgia&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can even turn it into an EPUB! Not that this would be necessary, but it is possible! I like epubs a lot, not really for scientific texts, but for normal books they are pleasant to use.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pandoc example.md --biblio ./bibtex/FRibliography_example.bib -o example.epub&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/media/figures/example_epub.png&quot; alt=&quot;epub&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now there are already some font issues rising their ugly heads…&lt;/p&gt;

&lt;p&gt;As the last thing I also convert the PDF into a picture used in the beginning of this post. It uses free program &lt;a href=&quot;http://www.imagemagick.org/script/index.php&quot;&gt;ImageMagick&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;convert -density 300  example.pdf example.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The script can be downloaded in one piece from &lt;a href=&quot;www.github.com/langdoc/GoogleDocs_with_md/compile_gdocs.R&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Jun 2015 01:55:14 +0200</pubDate>
        <link>http://langdoc.github.io/2015/06/22/google-docs.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/06/22/google-docs.html</guid>
        
        <category>R</category>
        
        <category>workflows</category>
        
        
      </item>
    
      <item>
        <title>Concatenating MTS files</title>
        <description>&lt;p&gt;We are having a good recording session in the village of Krasnoye, Nenetskiy Avtonomnyj Okrug, Northern Russia, when we realise that our new good video camera is filling up the sd-card much faster than we anticipated. Everything on the card is backed up to several external hard drives, I have one copy at my laptop even - let’s make there some space. It is not so good practice to touch your SD-cards, they are so cheap that they never really should need to be reused, but this situation demanded radical solutions. I go and remove some of the backupped .MTS files - that is the actual video file, right?&lt;/p&gt;

&lt;p&gt;Hell broke loose.&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The consequences were not evident very soon. However, the AVCHD file format is a bit more than the actual video files. I had naively thought that the files in the other folders must be for cameras internal work, thumbnails, ok, some playlist, I guess the camera needs those!&lt;/p&gt;

&lt;p&gt;When I started to edit the videos I realized this folder structure indeed kept some more information which was crucial for us. When the camera shoots the video, it automatically cuts the video into smaller pieces when they are large enough (with our settings that is something after half an hour, around 2,12 gigabytes per file). The way how these two files exactly combine, as far as I understand it now, is in .CPI files in CLIPINF folder.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:30.0 -c copy kpv_izva20150000-1-cut.MTS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the synchronization is succesful, the approach works, but we are clearly removing too much. It is a matter of just a few frames. The removal should not be detectable in the final result.&lt;/p&gt;

&lt;p&gt;We know that the video frame rate is 25 fps. Then one frame should be 0.04 seconds. After some testing I found that removing 16 first frames gives us a clean join. This doesn’t happen with any less frames being removed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:00.68 -c copy kpv_izva20150000-1-cut.MTS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Doing this processing takes around half a minute, so it is fast enough to test this multiple times with different configurations.&lt;/p&gt;

&lt;p&gt;However, when we open the file in Final Cut Pro X there is still something wrong. An annoying small green flash. However, it seems to be the case that we are somehow hitting the right spot with this cut, as any attempt to cut more seems to result in FCPX inserting black timeline between the clips. Now the clips join smoothly, which must be how it is supposed to be. Now we are not losing anything from the &lt;strong&gt;audio&lt;/strong&gt; track which is within those two videos. That small green flash is as long as three frames, that makes 0.12 seconds. What I ended up doing was to repeat the previous frame three times, in a configuration more or less like this:&lt;/p&gt;

&lt;p&gt;IMAGE HERE!&lt;/p&gt;

&lt;p&gt;This leaves to video tiny tiny portion where data is missing, but in the end of the day it is still manageable loss, at least not the worst that could happen with data mismanagement.&lt;/p&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;p&gt;What are we supposed to do with AVCHD files? That is the original raw data we get. Now it seems that the easiest way to access that data is through a commercial software such as Final Cut Pro X. This is fine, as we anyway have to use something like that to edit the files. However, is the resulting MOV file exactly the same as the original files in what comes to quality? What exactly happens with those files in AVCHD structure when they are opened in FCPX? There are some other open questions, mainly with archiving.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Should we archive the AVCHD files? If so, how they fit into our session structure? They are usually 30 GB files with lots of session in one large file. They also contain different binary files, would those be accepted as well?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How about archiving the session related Plural Eyes Project files (.pep) and Final Cut Pro XML (.fcpxml)? Those two files contain all information about the process how the raw files have been transformed into sessions ready to be transcribed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 21 May 2015 01:55:14 +0200</pubDate>
        <link>http://langdoc.github.io/2015/05/21/converting-mts-files.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/05/21/converting-mts-files.html</guid>
        
        
      </item>
    
      <item>
        <title>Moving files in Terminal</title>
        <description>&lt;p&gt;It is very common that we have a large amount of files in one folder, and then for whatever reason we decide that we would need to create for each filename a unique folder, where the different files sharing same name but different endings could reside. I had recently lots of ELAN files which I wanted to move into their own directories. I believe this is a very common situation, so I wanted to share it. &lt;!--more--&gt; It can be done with these three commands. Just copy paste them one by one to the Terminal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First command:&lt;/strong&gt; This makes the directories with the name of each ELAN file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls | grep &#39;\.eaf$&#39; | sed &#39;s/.eaf//g&#39; | xargs mkdir
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Second command:&lt;/strong&gt; This uses grep to pick all ELAN files, then creates variables for different things and repeats the move for each file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
for file in `ls | grep &#39;\.eaf$&#39;`
do
    SESSIONNAME=$(echo $file | sed &#39;s/.eaf//g&#39;)
    MOVFILE=&quot;$file&quot;
    MOVDESTINATION=&quot;./$SESSIONNAME/$file&quot;
    /bin/mv $MOVFILE $MOVDESTINATION;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Third command:&lt;/strong&gt; This does exactly the same as the second but for pfsx files.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
for file in `ls | grep &#39;\.pfsx$&#39;`
do
    SESSIONNAME=$(echo $file | sed &#39;s/.pfsx//g&#39;)
    MOVFILE=&quot;$file&quot;
    MOVDESTINATION=&quot;./$SESSIONNAME/$file&quot;
    /bin/mv $MOVFILE $MOVDESTINATION;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I did the same recently with a folder full of mp3 and wav files. The model is identical, just the filename differs. This can be a convenient way to rearrange messy file structure, just to start dragging everything into one folder and move onward with commands like these. This also forces to check that all files are consistently named, which is of course necessary for well structured corpus.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 May 2015 01:55:14 +0200</pubDate>
        <link>http://langdoc.github.io/2015/05/19/moving-files.html</link>
        <guid isPermaLink="true">http://langdoc.github.io/2015/05/19/moving-files.html</guid>
        
        <category>Terminal</category>
        
        
      </item>
    
  </channel>
</rss>
