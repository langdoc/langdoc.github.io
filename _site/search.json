[
  
    {
      "title"    : "Isogloss maps with Shapefiles",
      "category" : "",
      "tags"     : "R, maps",
      "url"      : "/2016/03/23/isogloss-maps.html",
      "date"     : "2016-03-23 18:00:00 +0100",
      "content"  : "I’ve been working lately a lot with language areas as different Shapefiles. This works nicely, and usually it is quite unproblematic to make a functional map. There are some problems I haven’t solved, mainly with the best way to store different attributes relevant for languages, but which I wouldn’t like to store repeatingly inside the Shapefile. But now I tackle a bit different issue, which is the use of Shapefiles to mark isoglosses. As usually, I don’t want to take the easiest route, but would look for something efficient, economic, pretty and easy to manage. Let’s take a look into a Shapefile, or Geojson, whatever. When the data is read to R as SpatialPolygonsDataFrame it behaves the same way whatever the origin. It is bit more complex format which has slots for different kinds of information. The most important ones are @data and @polygons. The first is just a data frame, and it can be accessed with object@data$column. It is possible to do there quite many of those things which you would do with any other data frame, which makes it very powerful tool.library(rgdal)library(dplyr)library(leaflet)kpv &lt;- readOGR("https://raw.githubusercontent.com/nikopartanen/language_maps/master/geojson/kpv.geojson", "OGRGeoJSON")koi &lt;- readOGR("https://raw.githubusercontent.com/nikopartanen/language_maps/master/geojson/koi.geojson", "OGRGeoJSON")jzv &lt;- readOGR("https://raw.githubusercontent.com/nikopartanen/language_maps/master/geojson/koi-j.geojson", "OGRGeoJSON")map &lt;- leaflet() %&gt;%              addProviderTiles("Esri.WorldTopoMap") %&gt;%    addPolygons(data = kpv,              fillOpacity = 0.4,               color = "gray",               weight = 1,              popup = ~dial) %&gt;%    addPolygons(data = koi,              fillOpacity = 0.4,               color = "gray",               weight = 1,              popup = ~dial) %&gt;%    addPolygons(data = jzv,              fillOpacity = 0.4,               color = "gray",               weight = 1,              popup = ~dial)Now there are of course lots of different isoglosses, but they don’t necessarily align by dialects. So we cannot just store information with what kind of isogloss value which polygon is associated. In a way where this line goes can be rather arbitrary, although it is also absolute in the sense that at least on more abstract level we can say this line goes along the line. It is always another question how the isoglosses pattern in our actual corpus data. But I think both representations have their own value and use.This is a good example which cuts across all main variants, showing the variation in stress within Komi. It is taken from Batalova’s 1982 book Ареальные исследования по восточным финно-угорским языкам (коми языки), page 41.To explain it simply, the majority of Komi-Zyrian has no morphologically significant stress, but in some southern dialects this feature is present, as it is in almost all Komi-Permyak, besides small eastern region where the stress is connected to vowel system more widely, as it is in all Komi-Yazva. In this case almost all action is in the southern varieties, so I’ll try to display it so that the attention is more theere (and some southern varieties are geographically very small, which makes them difficult to show on large scale maps).We could take the polygons above and modify a variant which is cut according to this pattern, but this has some problems as well. Mainly that the changes in original polygons would not be reflected in the Shapefiles which contain the isoglosses! This would have fast quite disastrous results, and generally speaking would not be very elegant.library(maptools)isogloss &lt;- readShapePoly("/Volumes/langdoc/maps/language_maps/isoglosses/kom.shp", proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))map %&gt;% addPolygons(data = isogloss,              fillOpacity = 0.1,               color = "gray",               weight = 3,              popup = ~value)This is maybe a bit strange approach. But in many ways it is the best I’ve come up with. We can use this command from GDAL to clip the isogloss polygon with the areas defined in the language polygon.ogr2ogr -clipsrc langs/kom/kom.shp isoglosses/kom_isogloss_1_cut.shp isoglosses/kom_isogloss.shpThis gives us something we can easily use elsewhere on maps. It will look as good as the language polygons do, but we can associate any kind of overlays with it and still be rather sure things are not breaking apart. The isogloss shapefiles will not need special maintenance work when the language polygons are modified, and this is something really important if the idea is to have something more reliable than just graphics used once and forgotten.It can be bit annoying ot modify Shapefiles like these, so take a look into snapping options in QGIS to make this bit easier. Otherwise it would be next to impossible to get the polygons align prettily, and I think it is quite crucial there are no holes between the polygons.There is nothing really new going on in the map below, but note that we are using RColorBrewer package to set up the color scheme. The colors are always little bit complicated. On one hand they should just appear automatically and be nice, but that doesn’t work so well with varying number of features, and at times some combinations just don’t work and you want to touch them manually. However, something with RColorBrewer should be simple enough to have inside a function which tries to select suitable color palette for most of the cases. And if there would be tens of different values to map, it probably would not be always that good idea to place them all on one map to start with.library(RColorBrewer)isogloss_cut &lt;- readShapePoly("/Volumes/langdoc/maps/language_maps/isoglosses/kom_isogloss_1_cut.shp", proj4string = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))value &lt;- as.character(distinct(isogloss_cut@data, value) %&gt;% arrange(value) %&gt;% .$value) colors &lt;- RColorBrewer::brewer.pal(length(value), "Dark2")leaflet() %&gt;% setView(lng = 54, lat = 60, zoom = 6) %&gt;%     addProviderTiles("Esri.WorldTopoMap") %&gt;%    addPolygons(data = isogloss_cut,              fillOpacity = 1,              fillColor = ~colors,              weight = 1,              popup = ~value,              stroke = .5) %&gt;%  addLegend("bottomright", colors = colors, labels = value,    title = "Stress in Komi Dialects",    opacity = 1)I think the next topic would be how to associate points with polygons, for example, to gather data from polygon layers to the token level data frames where there are different coordinates. It could be quite interesting to locate all speakers whose bithplace and place of residence are on different isogloss areas, as just looking to the adaptation of different variables could be enlightening."
    } ,
  
    {
      "title"    : "Adding shadows to images",
      "category" : "",
      "tags"     : "Terminal, maps",
      "url"      : "/2016/03/23/shadows-to-images.html",
      "date"     : "2016-03-23 10:56:00 +0100",
      "content"  : "I’m not certain if this is something anyone needs often, but at least I was surprised to learn this can also be done from the command line. So I needed to overlay something on a map as a marker, and already had fired up Gimp and started to edit a shadow file to be used in JavaScript. Little I knew that ImageMagick was able to do that as well!  So if one would start with a file like this:With command like this:convert Icon256x256.png '(' +clone -background black -shadow 80x100+100+100 ')' +swap -background none -layers merge +repage cat_shadowed.pngWe get a one with background shadow!You have to play with the numbers to get the direction, size and distance of the shadow right. I think the main potential use for this is in map icons."
    } ,
  
    {
      "title"    : "Generating sine-waves with R",
      "category" : "",
      "tags"     : "R, data",
      "url"      : "/2015/11/01/sine-waves-with-r.html",
      "date"     : "2015-11-01 14:56:00 +0100",
      "content"  : "This topic may not be immediately relevant, but can still be interesting for somebody. So the question is: How to generate sound files with R, which just play sine wave at one specific frequency. One immediate reason is to test the frequencies you are yourself able to hear. I’m now lost somewhere after 17250 hz, which sounds horrendously low, but maybe should not be so unusual for someone in my age (soon 29). # I load the packages explicitly here, and also refer to them # with their namespaces in individual functions.# I understand this is somewhat redundant, but I hope it makes # it more obvious which function is from which package.library(tuneR)library(seewave)library(plyr)# I needed to do this on Mac to get it work.# But please install Sox anyway, it is useful!tuneR::setWavPlayer("sox")# These are the frequencies we wantrates &lt;- c(100, 200, 300, 400, 800, 1000, 1200, 1400, 1600, 1800, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 15000, 15250, 15500, 15750, 16000, 16250, 16500, 16750, 17000, 17250, 17500, 17750, 18000, 20000)# This is a tiny function that in principle could also just be inside llply itself.# I find it easier to read this way when it is on its own and named.write_sines &lt;- function(x) {        seewave::savewav(tuneR::sine(x),                 filename = paste0("~/sounds/sound_", x ,"_hz.wav"))}# llply just repeats the function write_sines to all elements in rates character vectorplyr::llply(rates, write_sines)So basically this just fills the folder sounds with the files like sound_100_hz.wav and so on, as specified in the rates vector. Just modify the values and overwrite the old files. In principle one of course could had generated those numbers more automatically, but I think in this case some manual adjustment is good to be there, as you never know exactly which ones you or your ears need.Possible applications:  Have fun with your cats and high sounds  Your recording device usually would capture something up to 24 khz. This leaves us the range of 20-24 khz which is not really used as we humans can’t hear it. Is there anything on these higher frequencies one could do something with?"
    } ,
  
    {
      "title"    : "Data workflow for linguistic fieldwork",
      "category" : "",
      "tags"     : "fieldwork, data",
      "url"      : "/2015/09/16/session-workflow.html",
      "date"     : "2015-09-16 12:45:13 +0200",
      "content"  : "I think quite a lot has been written about the data management workflows in documentary linguistics. Lots of this is about the whole workflow from field to the archive. I want to document here the workflow I’ve personally found very good for the immediate work on the field, and also desire to use later. This doesn’t even touch the issues of annotating, archiving, publishing or disseminating. I want to stress that this all is something that in my opinion should be done already at the field. Period. Don’t delay it, the consequences are heavy.I’ve encountered occasionally an idea that taking part into a workflow such as presented here would be a sort of a choice. However, you are already there if you decide to involve both video and audio into your working practices. The moment you decide to bring a videocamera on the field dictates some aspects from your further workflow as well, as it has to take into account the existence of both audio and video data streams. Naturally it is possible to ignore the video, but in that case one could also ask what was it worth to do it in the first place.As if the situation would be just about the video. Several modern audio recorders, such as Edirol R-26, automatically produce as a number of audio files from different microphones. In order to work with this data in the most effective way we must somehow take this into account, as an example, by remixing the audio in way that brings up the best aspects of these distinct audio tracks, and recording in such a manner that we actually get the benefit that is there.The days of having just one track of audio are over. Are we winning or losing with these changes, that is to be seen. But not taking them into account is an ignorance we cannot afford.I assume that each part in the presented workflow will change in some point. The technology changes always, and there is no reason to stick into old if some better solution is around. This said, we should not end up using any solutions which we are not thoroughly familiar with. At the same time, it remains the fact that many of our tools are used the most while we are doing fieldwork, which also means that we learn very much about them on the field.The raw dataAs mentioned, the session raw data is getting wilder and wilder. Often we use several microphones, i.e. some lapel mics + one in a boompole to capture the overall sound. Also the camera makes its own audio track, which may be very usable in many cases. With many cameras this audio is also 6 channel surround, with which we have to consider are all those tracks containing what we want in the best possible constellation. Another thing to note is that many video cameras store their audio in Dolby Digital format which is not particularly transparent. Similarly, the video may be captured in AVCHD format, which also contains it’s own particularities.Generally speaking in documentary linguistics it has been getting more and more common to capture also video. This also turns it into something one is expected to produce. In principle using two cameras is not out of the question, as it solves many problems there inherently are with the camera angle. One can be used to capture the general overview of the setting, and this gives to the other one more freedom to move around and get experimental. This is where the human component comes in. When we start to zoom camera into something that could be interesting, there is always the danger that the interesting takes place somewhere else. If we use only one camera we have to be careful, but with two there is already more freedom.So a normal recording session contains very often at least the several files:  few audio files  few video files  scan from fieldnotes  metadata files  photographs of the informants  general photograph or screenshot from the recording setting itselfIn practise the metadata can often be in a database and not in the actual folder with the session raw data, but this is not really the point here.However, there are many steps before we actually can start to work with this data.Data processingThere are few steps one has to take with the raw data.  synchronization  mixing  cuttingThis is sort of a post-production phase which has not existed traditionally, but nowadays is there all the time. I’ve been using PluralEyes and Final Cut Pro X to do this. There are certainly other alternatives, but as far as I know there aren’t really open source solutions for this.PluralEyesWhat PluralEyes does is to take all the audio and video files and to look for matching segments in the audio spectrograms. This must be very similar to audio fingerprinting tools, used i.e. with Python library DejaVu. However, good side of PluralEyes is that it is easy and fast. It also exports the synchronization metadata as an XML file.Final Cut ProFinal Cut Pro is a commercial video editing software, used widely by different video professionals and amateurs alike. I guess Adobe Premiere does more or less the same job equally well. Also Final Cut Pro allows us to import and export session data as an XML file, by the way.In PluralEyes we don’t really do anything for the files, but in Final Cut Pro we can start to play more with them. Naturally, the question of cutting is very acute. Are we supposed to cut? What kind of videos are we doing as documentary linguists? Well, I guess the answer is that we are doing different videos for different uses. As an example, for a DVD that is sent to informant I may do different cuts than I do for the one that will be transcribed.At least we have good rationale to cut something away from the beginning and the end, as this often has some noise from setting up the tools and preparing for the session. The main reason to cut, in my opinion, is that often this part is captured by one of the devices, i.e. there is an audio track, but the video camera is just being set up. So it just makes it nicer if we leave that hassle out. On the other hand, this often contains very critical information about how the aims and goals of the session are explained to the informant, how the linguists negotiated among each others who stays where, how the room was rearranged to make recording work better etc. These things do influence the recording quite a lot, so it is very good if that part of the work is also documented. However, should that be part of the transcribed session? Maybe. Should that be sent to the informant who maybe just wants to have a nice DVD with the actual interview?Another example is that often the recording is interrupted and the technology is turned off for some time. As an example, we may turn camera and recorder off during a phone call or if someone has to leave for some personal errands. However, it is very important question can we just glue these two pieces together, or should we split these two into two different sessions? I’ve always split them into two, since I think the idea of a session is to represent one temporally consecutive situation. However, if the breaks are somehow very well indicated, I believe there is very little reason not to play around with these things if there is the need. One need would be to make an actually nice video from shorter video and audio recordings which are somehow topically interrelated. This is also nicer to transcribe and watch than treating each little file as their own session.Of course the Final Cut Pro XML files contain data about the ways how files are rearranged and cut.Note about XML filesNaturally it is one thing to archive the XML files produced by these programs and actually do something useful with them. Understanding their structure and actually getting information out from them is not always trivial either. There are no real applications at the moment that would really use this data, similarly, I don’t think it has been very common practise to archive XML files like these. However, I think the only way we can excuse the use of commercial software is to make certain that the data is not locked into these systems. In any points. Certainly, the most comfortable way to work with fcpxml files is to open them in Final Cut Pro. But we have to make sure that is not the only option.Audio mixingI must admit that I have no education about this topic. Maybe I should, as I’m a linguist who works with this stuff all the time, but I think there are very few instances of linguistic departments which include audio engineering as a part of their curricula.I sort of thought I had got it already work nicely, but then one musician friend of mine was listening some ELAN sessions and complained about an echo. Yes, indeed, several audio tracks played simultaneously make sort of a nice sound where everyone can be heard well, but there was indeed some small echo. I remixed the tracks differently, putting some more quiet, some louder, and the echo disappeared. Not that I would understand anything about the process, but clearly this is an enormous field someone should understand about.The situation is even more acute when it comes to work with old recordings. The sound quality is often quite bad, and there are certainly things we can do. I’ve used often ReaFir effect in REAPER to mute some frequencies, though there has been no way to include any audio editing software into the current workflow I have with documentary linguistic sessions. As our modern recordings are almost always very decent there is maybe little space for this, but still the question remains that mixing these audio channels from different devices is not trivial task at all.This also means that it has to be very well indicated which are the actual raw data files and which are mixed for actual session. Naturally for phonetic analysis one should consult the raw data, but what you get into ELAN is usually a mix of several audio files, I see no other alternative.Processed dataSo now the session folder actually contains already these files:  few audio files  few video files  scan from fieldnotes  metadata files  photograhps of the informants  general photograph or screenshot about the recording setting itself  PluralEyes XML  Final Cut Pro XML  audio file for ELAN  video file for ELAN  ELAN fileWhat I have been doing now is to create new folders for different outputs, i.e. I have folder called session_name-i for the file version that has been sent for the informant, that often contains just a DVD disc image. Then there are folders marked with a, b, and c etc. for distinct subsessions if they exist. Usually they do. One problem is that the session can be understood to work out so that if the beginning hassle counts as something of its own, then that’s the part a. Then the actual recording, if uninterrupted, gets the symbol b, and the end hassle is c. So then there is a very large amount of sessions with the symbol b in their name, which feels bit stupid at times, but doesn’t really matter so much. Maybe there are better ways to arrange these files.What’s the point?My idea about this workflow is such, that this would be ready for every session when we leave the fieldsite. If we have several video files, this version can be done with the most simple and general video there is. Don’t worry about nice video for the informants, that can be polished and sent back later. Quality wins over speed in that. For the later use it is possible to remix the video with different angles, this doesn’t influence the ELAN file of course in any way, unless there is actual mixing of the content, problems of which I already discussed briefly.I already keep track from all metadata on the field, and make sure that everything is arranged into their own folders and metadata is filled up, if possible, at the same evening. More days pass more difficult it gets to actually reconstruct what has happened. I think actually taking it as far as even exporting the media for ELAN files and creating those dummy ELAN files does make sense. If this video management workflow is not taken care of instantly, it will become an enormous burden for someone (I’ve been that someone for several times). Running through this workflow is not very slow in the end, the only slow part is exporting. So process the session data at the evening with a beer or the local mildly alcoholic beverage of choice, leave the computer export and backup over night, repeat. If you have a research assistant or someone else doing this, make sure they are unstressed and have the beverage they deserve, this is not easy work, but it is worth doing on the spot.I understand this workflow probably doesn’t work in locations where electricity is scarce. I work mainly in the Northern Russia, and there the villages are often very good locations for using computer at the evenings. I understand this is not always possible also time-wise, but my advice would be not to let this kind of work accumulate. I’ve spent months working with video and audio files from few weeks long fieldtrip, and very big part of that could had been ready even before I got back home.Is anything going to change?Maybe when the computers get faster also video exporting gets faster, though this is unlikely as the video probably gets larger at the same rate. Maybe in some point it gets convenient enough to make notes straight away in digital format. This would change a lot, if one could, as an example, timestamp the notes and associate them with the raw data.There has been this development that there is more and more fieldwork equipment. Two cameras means two tripods, and so on. However, there are some interesting developments. As an example, there have been lately more and more camera models which have a high quality objective in very small body. Naturally, one could assume that cameras like these are very good in video, as modern DSLR’s tend to be, and the small size makes them much more user-friendly. Now there are lots of hipstery retro models out there, but in some point we probably start to see products that could be immediately useful for documentary linguists as well. Just a note, in this workflow the video needs only some audio, it can be really bad even, just enough that it can be synchronized with the audio files.The majority of events we record tend to be interviews of some sort. They are in many ways near conversations, but there is often the setting that one tells and another asks. Even when there are multiple participants it is normal that someone takes a very central role and does the most of the speaking. It isn’t fully solved how to record more naturally occurring speech events, usually luck has something to do with that. I think one change in the thinking would be to realize that now we basically record people when they are supposed to say something. Instead we maybe should record people when they do something, and if we are lucky, they say something. This means that we would end up recording much more situations where no one actually says a thing. I’m not currently certain how to move forward with that, but I’m sure the coming years will be interesting.Of course it is very interesting if we would have recordings that last many hours and contain only few utterances. Why not. Hard disk space is cheap. These languages being used naturally is priceless."
    } ,
  
    {
      "title"    : "Editing text blocks in Emacs",
      "category" : "",
      "tags"     : "emacs, digitalization",
      "url"      : "/2015/09/01/emacs-editing.html",
      "date"     : "2015-09-01 13:06:13 +0200",
      "content"  : "Often when we digitalize some text collections we end up with text blocks in one language, and the translations on some other language in similar blocks. However, in our modern language collections the text is divided into utterances, which often more or less do match with sentences.There is no automatic way to match the sentences, because there isn’t always exact correspondence at the sentence level. However, some ways to work with this data are more difficult than the others.  One can start with some regular expressions such as these.The point of this is to mark paragraph breaks with the hash character."\n" &gt; "\n#\n"Then we can try to separate the sentences into their own lines."(\.|\,|\?|\!|\:) " &gt; "$1\n"This works quite well. But there is still mismatch, almost always, at least when we have a longer text. I do this often in TextMate, but any text editor can handle this simple things.Editing with EmacsI don’t use Emacs that often, though there are some situations where it is irreplaceable. One situation is this. We can open the two text blocks in two diagonally split buffers, and make our editing way easier with following commands in both buffers:M-x toggle-truncate-linesNow each line contains one string we want to match. Then let’s have line numbers:M-x linum-modeThen the last one, which actually makes all this effort worthwhile:M-x scroll-all-modeNow the both buffers are scrolling down and up simultaneously. With C-x-o we can move between the buffers, and by moving sideways between the lines it is easy to keep two trackers matching."
    } ,
  
    {
      "title"    : "Fieldwork map",
      "category" : "",
      "tags"     : "fieldwork, R",
      "url"      : "/2015/07/26/fieldwork-map.html",
      "date"     : "2015-07-26 01:55:14 +0200",
      "content"  : "I started to write some long rant about the data situation with semi-large language like Komi, and ended up combining it to geographical locations, with which I ended into mapping. For quite a while I had problems in embedding Leaflet/R based maps into Github Pages, although now when I realized how it works it is actually ridiculously easy. There are many frameworks to work with maps, but I think at the moment Leaflet is the one doing best. What kind of complicates the things is how one can use Leaflet within different frameworks, basically with JavaScript and with R. I have been working with both of them intensively, and started also collecting different language maps into a new Github repository called Language Maps.At the moment the map shows only Komi-Zyrian dialects. I’m on my way to connect it to our database and get some markers from there.just testing hereAlso more complicated work is possible, as an example, the map below shows the languages which are part of the INEL project I’m now working with.just testing here"
    } ,
  
    {
      "title"    : "Basic language maps",
      "category" : "",
      "tags"     : "maps, R, leaflet",
      "url"      : "/2015/07/26/basic-language-maps.html",
      "date"     : "2015-07-26 01:55:14 +0200",
      "content"  : "I started to write some long rant about the data situation with semi-large language like Komi, and ended up combining it to geographical locations, with which I ended into mapping. For quite a while I had problems in embedding Leaflet/R based maps into Github Pages, although now when I realized how it works it is actually ridiculously easy. There are many frameworks to work with maps, but I think at the moment Leaflet is the one doing best. What kind of complicates the things is how one can use Leaflet within different frameworks, basically with JavaScript and with R. I have been working with both of them intensively, and started also collecting different language maps into a new Github repository called Language Maps.I have this idea that we are not using geographical data as complexly as could.At the moment the map shows only Komi-Zyrian dialects. I’m on my way to connect it to our database and get some markers from there.Also more complicated work is possible, as an example, the map below shows the languages which are part of the INEL project I’m now working with."
    } ,
  
    {
      "title"    : "Google Docs and Markdown",
      "category" : "",
      "tags"     : "R, workflows",
      "url"      : "/2015/06/22/google-docs.html",
      "date"     : "2015-06-22 01:55:14 +0200",
      "content"  : "We were recently writing a small abstract together with a few colleagues (Michael Rießler and Hanna Thiele). We started in Dropbox, but that is, as usually, quite bad in intensive writing. In principle we could use other tools we use already anyway, GitHub or SVN, but the same problem remains that it would be very convenient to see what others are editing and use some clear system for commenting.I personally like to write this kind of simple things in Markdown. It has several advantages over LaTeX. It is easy, fast to type and allows good connection with bibtex bibliography. So I started to think whether there is some way to combine the best of both worlds. I know there are some other collaborative editors for Markdown itself, but there were some other reasons to stick to Google Docs. I have one colleague with who I collaborate regularly, who uses ChromeBook as his main computer. Thereby for him it is the best to stick into that. And this means that for me it is the best to figure out some ways to use Google Docs more effectively.The workflow presented here was inspired by this blogpost by Eric P. Green, though it is not at all that sophisticated.All code is stored in a GitHub repository, but I go it through here piece by piece. Here are the steps that come.  Create a YAML header  Download text from Google Docs  Remove mysterious NUL character from the beginning of file  Delete the comments that may be present  Combine the header and the text  Write those into a markdown document  Render HTML  Render PDFIt is really very simple. The idea is to read data directly from Google Docs. And it allows this:To be turned into something like this:Or into an HTML page, or into .docx file, or .epub, or basically into anything Pandoc supports. So let’s go through step by step how this works.The headerHeader is a simple block of YAML. We are telling it the date, author, title, the location of a bibliography and other such details. It depends a bit from our desired output format what we want to have here, as we can also pass many of these settings directly into Pandoc. Here we just save the header as a character string.header &lt;- '---title: "Collaborative authoring in Google Docs"author: "Niko Partanen"date: "19 Jun 2015"pdf_document:        keep_tex: yesbibliography: ./bibtex/FRibliography_example.bib---'Reading the fileThen we read the file from Google Docs and write it as a file. The link to Google Docs is very easy to generate. You can get the document ID from the link you use to share it.The structure is like this:https://docs.google.com/document/d/put-id-here/export?format=txtOne could have there also some other format, but a simple text file is the best for now.library(readr)doc &lt;- read_file("https://docs.google.com/document/d/1D1C_79nLDM25r96dWinufVUd70-ipsHzv6cK-9cK-Io/export?format=txt")fileConn &lt;- file("temp.txt")writeLines(doc, fileConn)close(fileConn)Removing the NUL characterFor some reason the text file contains a NUL character in the beginning of the first line. I don’t understand at all what it is doing there, but R, LaTeX and Pandoc are all giving errors for it later, so having it around is unacceptable. I was trying to find out how to delete it for quite some time, and ended up to do it from Terminal, by simply removing those first bytes.system("cat temp.txt | tail -c +4 &gt; temp_clean.txt")doc &lt;- read_file("temp_clean.txt")I would be very happy to know if there is some easier way to do this!Removing the commentsIt is possible that there are some comments left into the document. They have formatting like [a] or [b] in the text, and the actual comment text comes to the end. I assume there could be some way to turn them into footnotes in the PDF, but at the moment I just want to delete them.doc &lt;- gsub("(.+## References).+", "\\1", doc)doc &lt;- gsub("\\[.\\]", "", doc)Putting it togetherThen we simply paste together the header and the document. This is then written into a new Markdown document.full_paper &lt;- paste0(header, doc)fileConn &lt;- file("example.md")writeLines(full_paper, fileConn)close(fileConn)Converting the filesAfter this we can start to convert the file. Now I convert it into HTML and PDF. Note for linguists: The default fonts used here are not very good for any exotic script. There are lots of situations where the basic fonts in principle can handle what you have there, but in some specific situations (such as within a code block) the font in that environment doesn’t have what you need. Well, this is how the things always are with typography and lesser used languages. But remember to play with Pandoc font settings if you have any non-latin stuff there.library(rmarkdown)render('example.md', output_format = "html_document")system("pandoc example.md --latex-engine=xelatex --biblio ./bibtex/FRibliography_example.bib -o example.pdf --variable mainfont=Georgia")We can even turn it into an EPUB! Not that this would be necessary, but it is possible! I like epubs a lot, not really for scientific texts, but for normal books they are pleasant to use.system("pandoc example.md --biblio ./bibtex/FRibliography_example.bib -o example.epub")Now there are already some font issues rising their ugly heads…As the last thing I also convert the PDF into a picture used in the beginning of this post. It uses free program ImageMagick.system("convert -density 300  example.pdf example.png")The script can be downloaded in one piece from here."
    } ,
  
    {
      "title"    : "Concatenating MTS files",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/05/21/converting-mts-files.html",
      "date"     : "2015-05-21 01:55:14 +0200",
      "content"  : "We are having a good recording session in the village of Krasnoye, Nenetskiy Avtonomnyj Okrug, Northern Russia, when we realise that our new good video camera is filling up the sd-card much faster than we anticipated. Everything on the card is backed up to several external hard drives, I have one copy at my laptop even - let’s make there some space. It is not so good practice to touch your SD-cards, they are so cheap that they never really should need to be reused, but this situation demanded radical solutions. I go and remove some of the backupped .MTS files - that is the actual video file, right?Hell broke loose.The consequences were not evident very soon. However, the AVCHD file format is a bit more than the actual video files. I had naively thought that the files in the other folders must be for cameras internal work, thumbnails, ok, some playlist, I guess the camera needs those!When I started to edit the videos I realized this folder structure indeed kept some more information which was crucial for us. When the camera shoots the video, it automatically cuts the video into smaller pieces when they are large enough (with our settings that is something after half an hour, around 2,12 gigabytes per file). The way how these two files exactly combine, as far as I understand it now, is in .CPI files in CLIPINF folder.ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:30.0 -c copy kpv_izva20150000-1-cut.MTSNow the synchronization is succesful, the approach works, but we are clearly removing too much. It is a matter of just a few frames. The removal should not be detectable in the final result.We know that the video frame rate is 25 fps. Then one frame should be 0.04 seconds. After some testing I found that removing 16 first frames gives us a clean join. This doesn’t happen with any less frames being removed:ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:00.68 -c copy kpv_izva20150000-1-cut.MTSDoing this processing takes around half a minute, so it is fast enough to test this multiple times with different configurations.However, when we open the file in Final Cut Pro X there is still something wrong. An annoying small green flash. However, it seems to be the case that we are somehow hitting the right spot with this cut, as any attempt to cut more seems to result in FCPX inserting black timeline between the clips. Now the clips join smoothly, which must be how it is supposed to be. Now we are not losing anything from the audio track which is within those two videos. That small green flash is as long as three frames, that makes 0.12 seconds. What I ended up doing was to repeat the previous frame three times, in a configuration more or less like this:IMAGE HERE!This leaves to video tiny tiny portion where data is missing, but in the end of the day it is still manageable loss, at least not the worst that could happen with data mismanagement.QuestionsWhat are we supposed to do with AVCHD files? That is the original raw data we get. Now it seems that the easiest way to access that data is through a commercial software such as Final Cut Pro X. This is fine, as we anyway have to use something like that to edit the files. However, is the resulting MOV file exactly the same as the original files in what comes to quality? What exactly happens with those files in AVCHD structure when they are opened in FCPX? There are some other open questions, mainly with archiving.      Should we archive the AVCHD files? If so, how they fit into our session structure? They are usually 30 GB files with lots of session in one large file. They also contain different binary files, would those be accepted as well?        How about archiving the session related Plural Eyes Project files (.pep) and Final Cut Pro XML (.fcpxml)? Those two files contain all information about the process how the raw files have been transformed into sessions ready to be transcribed.  "
    } ,
  
    {
      "title"    : "Moving files in Terminal",
      "category" : "",
      "tags"     : "Terminal",
      "url"      : "/2015/05/19/moving-files.html",
      "date"     : "2015-05-19 01:55:14 +0200",
      "content"  : "It is very common that we have a large amount of files in one folder, and then for whatever reason we decide that we would need to create for each filename a unique folder, where the different files sharing same name but different endings could reside. I had recently lots of ELAN files which I wanted to move into their own directories. I believe this is a very common situation, so I wanted to share it.  It can be done with these three commands. Just copy paste them one by one to the Terminal.First command: This makes the directories with the name of each ELAN filels | grep '\.eaf$' | sed 's/.eaf//g' | xargs mkdirSecond command: This uses grep to pick all ELAN files, then creates variables for different things and repeats the move for each file#!/bin/bashfor file in `ls | grep '\.eaf$'`do    SESSIONNAME=$(echo $file | sed 's/.eaf//g')    MOVFILE="$file"    MOVDESTINATION="./$SESSIONNAME/$file"    /bin/mv $MOVFILE $MOVDESTINATION;doneThird command: This does exactly the same as the second but for pfsx files.#!/bin/bashfor file in `ls | grep '\.pfsx$'`do    SESSIONNAME=$(echo $file | sed 's/.pfsx//g')    MOVFILE="$file"    MOVDESTINATION="./$SESSIONNAME/$file"    /bin/mv $MOVFILE $MOVDESTINATION;doneI did the same recently with a folder full of mp3 and wav files. The model is identical, just the filename differs. This can be a convenient way to rearrange messy file structure, just to start dragging everything into one folder and move onward with commands like these. This also forces to check that all files are consistently named, which is of course necessary for well structured corpus."
    } ,
  
    {
      "title"    : "Spectrograms with Spek",
      "category" : "",
      "tags"     : "audio, recording",
      "url"      : "/2015/05/19/spectrograms-with-spek.html",
      "date"     : "2015-05-19 01:55:14 +0200",
      "content"  : "One of the problems in ELAN is that it doesn’t really give you a good picture from the wave form for the whole file. Often this is full of visual cues that help us to find relevant pieces, i.e. songs, where new section starts etc. Naturally for different purposes we need different views, and this means use of different programs which handle some aspect particularly well.To get overview into the file, I’d say that among free programs Audacity does this well, Reaper is another good low-cost option, but I think the way Audacity produces the view is the most telling. (I bet one can produce waveforms also directly from R with Seewave R package, but this is an experiment for another time.)One thing we would need every now and then is to have very clean spectrograms from the audio in order to see what is really going on at the different frequencies, especially the highest ones. This tells us a lot about the background noices of the recording space.This example is from one session which was done indoors in a quiet environment, closed door, closed windows. But there was some periodic work going on outside which we didn’t really hear through closed window, but it was there. Some sort of machinery operating pretty far away from us, but still apparently close enough for microphones to catch something from it.The pattern here reminds the one produced by every fieldworkers enemy number one: the Russian refridgerator. This is something less problematic, not really even audible in itself. Ideally we would have view like this into our recordings while they are being done.This also reminds from Nyquist–Shannon sampling theorem. It states that the highest frequency that can be captured is approximately half from the sampling rate chosen. As our sampling rate is always 48 kHz, it means that we would capture something up to 20+ kHz. This seems to be the case. And it is of course reasonable as we capture basically the range a human can hear and a bit extra.What would happen if the sampling rate would be higher? As a very young man I was doing fieldwork in Karelia, and indeed some time had set the sampling rate into 96 kHz. I must have been thinking that if I have the settings high enough at least I can’t screw it up! Well, I guess there is no harm done, only wasted hard disk space.Suddenly there are higher frequencies! And this must be a consequence of higher sampling rate. These both files were recorded with Edirol HR-09 recorder, but the first one with an external lapel mic. That explains the exceptional clarity of the signal.In principle, if one would record in a cave full of bats, then would their high-frequency noise appear as distortion on 48 kHz recording but be captured adequately at the 96 kHz recording? The problem is that those high frequency sounds are present in the lower sampling rate too, they just are not there so systematically that their actual form would had been captured! Something to think about (yet nothing to act upon, this is a theoretical question)!Things to do  Check what is going on at the Praat level of view with higher frequency noises on and off  Simulate the environment with lots of above-hearing-level frequency noise and see what happens to the recordings"
    } ,
  
    {
      "title"    : "Welcome to the blog",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/05/17/langdoc.html",
      "date"     : "2015-05-17 01:55:14 +0200",
      "content"  : "While we document the endangered languages of the world there is quite a lot to do. Data quality, data content, connection with the speech communities, managing different audio and video files – this is in many ways extremely difficult job to do. One would assume that the technical solutions which we use would had been designed to make this work as easy as possible. Somehow this does not seem to be the case. Technology is just a tool, our real questions have nothing to do with it. This blog functions mainly as a platform where one can collect notes and tips about how to do different things.How often do we update our archived data? I think it is often done maybe every half a year, just because the uploading process is so inconvenient, slow and unreliable. I’m certain this is not intentional. Surely everyone has done their part well and the tools are designed to work in the best possible manner. But something has gone wrong.Filling metadata should be so easy that one does it just for joy of keeping things ordered the easiest possible way. Similarly, archiving should be so easy that one just syncs the newest files in the end of the day. Press a button, collect your stuff, tidy your desk – beep – sync done. Repeat every day you do something with your data.This should be possible with current technology. We believe this doesn’t need to be so difficult. There are better solutions for almost any aspect of our digital data management, and we are going to figure out what those are.At times the tools we need do not exist. But then we have to build them. Luckily, programming nowadays has started to remind more and more building with Legos. There are so many packages and modules for all kinds of languages (R, Python, Ruby – whatever one needs) floating around GitHub and other repositories that often 70% of the work is already done. It’s just about combining those pieces together in a way that contributes to language documentation workflow.This site, LangDoc, has been built in order to disseminate our ideas and practices. It is divided into individual blogposts. We also plan having independent daughter sites for specific projects and datasets."
    } 
  
  ,
  
   {
     
        "title"    : "About",
        "category" : "",
        "tags"     : "",
        "url"      : "/about/",
        "date"     : "",
        "content"  : "You can find the source code of this page from github.com/langdoc. Our goal is that LangDoc site would gather different modern solutions to language documentation, and help to disseminate the best practices.Idea is that the tools for different workflows would be directly accessible in their own GitHub repositories. This also allows more collaborative work and discussion around them, as an example, in the form of GitHub issues.Please contact us if you have any questions. The contact information is below.LicenceCC-BY"
     
   } ,
  
   {
     
        "title"    : "Categories",
        "category" : "",
        "tags"     : "",
        "url"      : "/categories.html",
        "date"     : "",
        "content"  : "Categoriesaudio                                                  Spectrograms with Spek                      recording                                                  Spectrograms with Spek                      Terminal                                                  Adding shadows to images                          Moving files in Terminal                      R                                                  Isogloss maps with Shapefiles                          Generating sine-waves with R                          Fieldwork map                          Basic language maps                          Google Docs and Markdown                      workflows                                                  Google Docs and Markdown                      maps                                                  Isogloss maps with Shapefiles                          Adding shadows to images                          Basic language maps                      leaflet                                                  Basic language maps                      fieldwork                                                  Data workflow for linguistic fieldwork                          Fieldwork map                      emacs                                                  Editing text blocks in Emacs                      digitalization                                                  Editing text blocks in Emacs                      data                                                  Generating sine-waves with R                          Data workflow for linguistic fieldwork                      "
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
        "title"    : "Search",
        "category" : "",
        "tags"     : "",
        "url"      : "/search/",
        "date"     : "",
        "content"  : ""
     
   } ,
  
   {
     
   } ,
  
   {
     
   } 
  
]
