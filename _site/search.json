[
  
    {
      "title"    : "Adding shadows to images",
      "category" : "",
      "tags"     : "Terminal, maps",
      "url"      : "/2016/03/23/shadows-to-images.html",
      "date"     : "2016-03-23 10:56:00 +0100",
      "content"  : "I’m not certain if this is something anyone needs often, but at least I was surprised to learn this can also be done from the command line. So I needed to overlay something on a map as a marker, and already had fired up Gimp and started to edit a shadow file to be used in JavaScript. Little I knew that ImageMagick was able to do that as well!  So if one would start with a file like this:With command like this:convert Icon256x256.png '(' +clone -background black -shadow 80x100+100+100 ')' +swap -background none -layers merge +repage cat_shadowed.pngWe get a one with background shadow!![shadow_example(/media/figures/cat_shadowed.png)]You have to play with the numbers to get the direction, size and distance of the shadow right. I think the main potential use for this is in map icons."
    } ,
  
    {
      "title"    : "Generating sine-waves with R",
      "category" : "",
      "tags"     : "R, data",
      "url"      : "/2015/11/01/sine-waves-with-r.html",
      "date"     : "2015-11-01 14:56:00 +0100",
      "content"  : "This topic may not be immediately relevant, but can still be interesting for somebody. So the question is: How to generate sound files with R, which just play sine wave at one specific frequency. One immediate reason is to test the frequencies you are yourself able to hear. I’m now lost somewhere after 17250 hz, which sounds horrendously low, but maybe should not be so unusual for someone in my age (soon 29). # I load the packages explicitly here, and also refer to them # with their namespaces in individual functions.# I understand this is somewhat redundant, but I hope it makes # it more obvious which function is from which package.library(tuneR)library(seewave)library(plyr)# I needed to do this on Mac to get it work.# But please install Sox anyway, it is useful!tuneR::setWavPlayer("sox")# These are the frequencies we wantrates &lt;- c(100, 200, 300, 400, 800, 1000, 1200, 1400, 1600, 1800, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 15000, 15250, 15500, 15750, 16000, 16250, 16500, 16750, 17000, 17250, 17500, 17750, 18000, 20000)# This is a tiny function that in principle could also just be inside llply itself.# I find it easier to read this way when it is on its own and named.write_sines &lt;- function(x) {        seewave::savewav(tuneR::sine(x),                 filename = paste0("~/sounds/sound_", x ,"_hz.wav"))}# llply just repeats the function write_sines to all elements in rates character vectorplyr::llply(rates, write_sines)So basically this just fills the folder sounds with the files like sound_100_hz.wav and so on, as specified in the rates vector. Just modify the values and overwrite the old files. In principle one of course could had generated those numbers more automatically, but I think in this case some manual adjustment is good to be there, as you never know exactly which ones you or your ears need.Possible applications:  Have fun with your cats and high sounds  Your recording device usually would capture something up to 24 khz. This leaves us the range of 20-24 khz which is not really used as we humans can’t hear it. Is there anything on these higher frequencies one could do something with?"
    } ,
  
    {
      "title"    : "Data workflow for linguistic fieldwork",
      "category" : "",
      "tags"     : "fieldwork, data",
      "url"      : "/2015/09/16/session-workflow.html",
      "date"     : "2015-09-16 12:45:13 +0200",
      "content"  : "I think quite a lot has been written about the data management workflows in documentary linguistics. Lots of this is about the whole workflow from field to the archive. I want to document here the workflow I’ve personally found very good for the immediate work on the field, and also desire to use later. This doesn’t even touch the issues of annotating, archiving, publishing or disseminating. I want to stress that this all is something that in my opinion should be done already at the field. Period. Don’t delay it, the consequences are heavy.I’ve encountered occasionally an idea that taking part into a workflow such as presented here would be a sort of a choice. However, you are already there if you decide to involve both video and audio into your working practices. The moment you decide to bring a videocamera on the field dictates some aspects from your further workflow as well, as it has to take into account the existence of both audio and video data streams. Naturally it is possible to ignore the video, but in that case one could also ask what was it worth to do it in the first place.As if the situation would be just about the video. Several modern audio recorders, such as Edirol R-26, automatically produce as a number of audio files from different microphones. In order to work with this data in the most effective way we must somehow take this into account, as an example, by remixing the audio in way that brings up the best aspects of these distinct audio tracks, and recording in such a manner that we actually get the benefit that is there.The days of having just one track of audio are over. Are we winning or losing with these changes, that is to be seen. But not taking them into account is an ignorance we cannot afford.I assume that each part in the presented workflow will change in some point. The technology changes always, and there is no reason to stick into old if some better solution is around. This said, we should not end up using any solutions which we are not thoroughly familiar with. At the same time, it remains the fact that many of our tools are used the most while we are doing fieldwork, which also means that we learn very much about them on the field.The raw dataAs mentioned, the session raw data is getting wilder and wilder. Often we use several microphones, i.e. some lapel mics + one in a boompole to capture the overall sound. Also the camera makes its own audio track, which may be very usable in many cases. With many cameras this audio is also 6 channel surround, with which we have to consider are all those tracks containing what we want in the best possible constellation. Another thing to note is that many video cameras store their audio in Dolby Digital format which is not particularly transparent. Similarly, the video may be captured in AVCHD format, which also contains it’s own particularities.Generally speaking in documentary linguistics it has been getting more and more common to capture also video. This also turns it into something one is expected to produce. In principle using two cameras is not out of the question, as it solves many problems there inherently are with the camera angle. One can be used to capture the general overview of the setting, and this gives to the other one more freedom to move around and get experimental. This is where the human component comes in. When we start to zoom camera into something that could be interesting, there is always the danger that the interesting takes place somewhere else. If we use only one camera we have to be careful, but with two there is already more freedom.So a normal recording session contains very often at least the several files:  few audio files  few video files  scan from fieldnotes  metadata files  photographs of the informants  general photograph or screenshot from the recording setting itselfIn practise the metadata can often be in a database and not in the actual folder with the session raw data, but this is not really the point here.However, there are many steps before we actually can start to work with this data.Data processingThere are few steps one has to take with the raw data.  synchronization  mixing  cuttingThis is sort of a post-production phase which has not existed traditionally, but nowadays is there all the time. I’ve been using PluralEyes and Final Cut Pro X to do this. There are certainly other alternatives, but as far as I know there aren’t really open source solutions for this.PluralEyesWhat PluralEyes does is to take all the audio and video files and to look for matching segments in the audio spectrograms. This must be very similar to audio fingerprinting tools, used i.e. with Python library DejaVu. However, good side of PluralEyes is that it is easy and fast. It also exports the synchronization metadata as an XML file.Final Cut ProFinal Cut Pro is a commercial video editing software, used widely by different video professionals and amateurs alike. I guess Adobe Premiere does more or less the same job equally well. Also Final Cut Pro allows us to import and export session data as an XML file, by the way.In PluralEyes we don’t really do anything for the files, but in Final Cut Pro we can start to play more with them. Naturally, the question of cutting is very acute. Are we supposed to cut? What kind of videos are we doing as documentary linguists? Well, I guess the answer is that we are doing different videos for different uses. As an example, for a DVD that is sent to informant I may do different cuts than I do for the one that will be transcribed.At least we have good rationale to cut something away from the beginning and the end, as this often has some noise from setting up the tools and preparing for the session. The main reason to cut, in my opinion, is that often this part is captured by one of the devices, i.e. there is an audio track, but the video camera is just being set up. So it just makes it nicer if we leave that hassle out. On the other hand, this often contains very critical information about how the aims and goals of the session are explained to the informant, how the linguists negotiated among each others who stays where, how the room was rearranged to make recording work better etc. These things do influence the recording quite a lot, so it is very good if that part of the work is also documented. However, should that be part of the transcribed session? Maybe. Should that be sent to the informant who maybe just wants to have a nice DVD with the actual interview?Another example is that often the recording is interrupted and the technology is turned off for some time. As an example, we may turn camera and recorder off during a phone call or if someone has to leave for some personal errands. However, it is very important question can we just glue these two pieces together, or should we split these two into two different sessions? I’ve always split them into two, since I think the idea of a session is to represent one temporally consecutive situation. However, if the breaks are somehow very well indicated, I believe there is very little reason not to play around with these things if there is the need. One need would be to make an actually nice video from shorter video and audio recordings which are somehow topically interrelated. This is also nicer to transcribe and watch than treating each little file as their own session.Of course the Final Cut Pro XML files contain data about the ways how files are rearranged and cut.Note about XML filesNaturally it is one thing to archive the XML files produced by these programs and actually do something useful with them. Understanding their structure and actually getting information out from them is not always trivial either. There are no real applications at the moment that would really use this data, similarly, I don’t think it has been very common practise to archive XML files like these. However, I think the only way we can excuse the use of commercial software is to make certain that the data is not locked into these systems. In any points. Certainly, the most comfortable way to work with fcpxml files is to open them in Final Cut Pro. But we have to make sure that is not the only option.Audio mixingI must admit that I have no education about this topic. Maybe I should, as I’m a linguist who works with this stuff all the time, but I think there are very few instances of linguistic departments which include audio engineering as a part of their curricula.I sort of thought I had got it already work nicely, but then one musician friend of mine was listening some ELAN sessions and complained about an echo. Yes, indeed, several audio tracks played simultaneously make sort of a nice sound where everyone can be heard well, but there was indeed some small echo. I remixed the tracks differently, putting some more quiet, some louder, and the echo disappeared. Not that I would understand anything about the process, but clearly this is an enormous field someone should understand about.The situation is even more acute when it comes to work with old recordings. The sound quality is often quite bad, and there are certainly things we can do. I’ve used often ReaFir effect in REAPER to mute some frequencies, though there has been no way to include any audio editing software into the current workflow I have with documentary linguistic sessions. As our modern recordings are almost always very decent there is maybe little space for this, but still the question remains that mixing these audio channels from different devices is not trivial task at all.This also means that it has to be very well indicated which are the actual raw data files and which are mixed for actual session. Naturally for phonetic analysis one should consult the raw data, but what you get into ELAN is usually a mix of several audio files, I see no other alternative.Processed dataSo now the session folder actually contains already these files:  few audio files  few video files  scan from fieldnotes  metadata files  photograhps of the informants  general photograph or screenshot about the recording setting itself  PluralEyes XML  Final Cut Pro XML  audio file for ELAN  video file for ELAN  ELAN fileWhat I have been doing now is to create new folders for different outputs, i.e. I have folder called session_name-i for the file version that has been sent for the informant, that often contains just a DVD disc image. Then there are folders marked with a, b, and c etc. for distinct subsessions if they exist. Usually they do. One problem is that the session can be understood to work out so that if the beginning hassle counts as something of its own, then that’s the part a. Then the actual recording, if uninterrupted, gets the symbol b, and the end hassle is c. So then there is a very large amount of sessions with the symbol b in their name, which feels bit stupid at times, but doesn’t really matter so much. Maybe there are better ways to arrange these files.What’s the point?My idea about this workflow is such, that this would be ready for every session when we leave the fieldsite. If we have several video files, this version can be done with the most simple and general video there is. Don’t worry about nice video for the informants, that can be polished and sent back later. Quality wins over speed in that. For the later use it is possible to remix the video with different angles, this doesn’t influence the ELAN file of course in any way, unless there is actual mixing of the content, problems of which I already discussed briefly.I already keep track from all metadata on the field, and make sure that everything is arranged into their own folders and metadata is filled up, if possible, at the same evening. More days pass more difficult it gets to actually reconstruct what has happened. I think actually taking it as far as even exporting the media for ELAN files and creating those dummy ELAN files does make sense. If this video management workflow is not taken care of instantly, it will become an enormous burden for someone (I’ve been that someone for several times). Running through this workflow is not very slow in the end, the only slow part is exporting. So process the session data at the evening with a beer or the local mildly alcoholic beverage of choice, leave the computer export and backup over night, repeat. If you have a research assistant or someone else doing this, make sure they are unstressed and have the beverage they deserve, this is not easy work, but it is worth doing on the spot.I understand this workflow probably doesn’t work in locations where electricity is scarce. I work mainly in the Northern Russia, and there the villages are often very good locations for using computer at the evenings. I understand this is not always possible also time-wise, but my advice would be not to let this kind of work accumulate. I’ve spent months working with video and audio files from few weeks long fieldtrip, and very big part of that could had been ready even before I got back home.Is anything going to change?Maybe when the computers get faster also video exporting gets faster, though this is unlikely as the video probably gets larger at the same rate. Maybe in some point it gets convenient enough to make notes straight away in digital format. This would change a lot, if one could, as an example, timestamp the notes and associate them with the raw data.There has been this development that there is more and more fieldwork equipment. Two cameras means two tripods, and so on. However, there are some interesting developments. As an example, there have been lately more and more camera models which have a high quality objective in very small body. Naturally, one could assume that cameras like these are very good in video, as modern DSLR’s tend to be, and the small size makes them much more user-friendly. Now there are lots of hipstery retro models out there, but in some point we probably start to see products that could be immediately useful for documentary linguists as well. Just a note, in this workflow the video needs only some audio, it can be really bad even, just enough that it can be synchronized with the audio files.The majority of events we record tend to be interviews of some sort. They are in many ways near conversations, but there is often the setting that one tells and another asks. Even when there are multiple participants it is normal that someone takes a very central role and does the most of the speaking. It isn’t fully solved how to record more naturally occurring speech events, usually luck has something to do with that. I think one change in the thinking would be to realize that now we basically record people when they are supposed to say something. Instead we maybe should record people when they do something, and if we are lucky, they say something. This means that we would end up recording much more situations where no one actually says a thing. I’m not currently certain how to move forward with that, but I’m sure the coming years will be interesting.Of course it is very interesting if we would have recordings that last many hours and contain only few utterances. Why not. Hard disk space is cheap. These languages being used naturally is priceless."
    } ,
  
    {
      "title"    : "Editing text blocks in Emacs",
      "category" : "",
      "tags"     : "emacs, digitalization",
      "url"      : "/2015/09/01/emacs-editing.html",
      "date"     : "2015-09-01 13:06:13 +0200",
      "content"  : "Often when we digitalize some text collections we end up with text blocks in one language, and the translations on some other language in similar blocks. However, in our modern language collections the text is divided into utterances, which often more or less do match with sentences.There is no automatic way to match the sentences, because there isn’t always exact correspondence at the sentence level. However, some ways to work with this data are more difficult than the others.  One can start with some regular expressions such as these.The point of this is to mark paragraph breaks with the hash character."\n" &gt; "\n#\n"Then we can try to separate the sentences into their own lines."(\.|\,|\?|\!|\:) " &gt; "$1\n"This works quite well. But there is still mismatch, almost always, at least when we have a longer text. I do this often in TextMate, but any text editor can handle this simple things.Editing with EmacsI don’t use Emacs that often, though there are some situations where it is irreplaceable. One situation is this. We can open the two text blocks in two diagonally split buffers, and make our editing way easier with following commands in both buffers:M-x toggle-truncate-linesNow each line contains one string we want to match. Then let’s have line numbers:M-x linum-modeThen the last one, which actually makes all this effort worthwhile:M-x scroll-all-modeNow the both buffers are scrolling down and up simultaneously. With C-x-o we can move between the buffers, and by moving sideways between the lines it is easy to keep two trackers matching."
    } ,
  
    {
      "title"    : "Fieldwork map",
      "category" : "",
      "tags"     : "fieldwork, R",
      "url"      : "/2015/07/26/fieldwork-map.html",
      "date"     : "2015-07-26 01:55:14 +0200",
      "content"  : "Doing language documentation with a language like Komi is in many ways different from working with actually non-documented languages. With Komi there is a long research history and collection of data has been ongoing for about 170 years. The data varies and is uneven, but the new data collected now can only be observed and used in connection to the old. In no village can we ever be the first fieldworkers. This of course opens up the question of how many villages there are and where exactly they are located.The map below shows the fieldwork done within Iźva Komi and some other Komi dialects by different researchers. This is a simple beta-version, which doesn’t contain everything. Complete version will be soon exported from our database, and additions are welcome.It must be mentioned that this map is heavily biased to the work done by West-European scholars. Naturally, native Komi researchers have been working enormously for decades with different Komi varieties. This bias is simply result of situation where data stored in the Western archives is more accessible for us than what is stored in local Russian archives.At the moment the shows only Komi-Zyrian dialects. I’m on my way to connect it to our database and get some markers from there.&lt;iframe id=”map_id” width=700 height=500 src=”http://nikopartanen.github.io/language_maps/kpv_dial.html”&gt;&lt;/iframe&gt;"
    } ,
  
    {
      "title"    : "Google Docs and Markdown",
      "category" : "",
      "tags"     : "R, workflows",
      "url"      : "/2015/06/22/google-docs.html",
      "date"     : "2015-06-22 01:55:14 +0200",
      "content"  : "We were recently writing a small abstract together with a few colleagues (Michael Rießler and Hanna Thiele). We started in Dropbox, but that is, as usually, quite bad in intensive writing. In principle we could use other tools we use already anyway, GitHub or SVN, but the same problem remains that it would be very convenient to see what others are editing and use some clear system for commenting.I personally like to write this kind of simple things in Markdown. It has several advantages over LaTeX. It is easy, fast to type and allows good connection with bibtex bibliography. So I started to think whether there is some way to combine the best of both worlds. I know there are some other collaborative editors for Markdown itself, but there were some other reasons to stick to Google Docs. I have one colleague with who I collaborate regularly, who uses ChromeBook as his main computer. Thereby for him it is the best to stick into that. And this means that for me it is the best to figure out some ways to use Google Docs more effectively.The workflow presented here was inspired by this blogpost by Eric P. Green, though it is not at all that sophisticated.All code is stored in a GitHub repository, but I go it through here piece by piece. Here are the steps that come.  Create a YAML header  Download text from Google Docs  Remove mysterious NUL character from the beginning of file  Delete the comments that may be present  Combine the header and the text  Write those into a markdown document  Render HTML  Render PDFIt is really very simple. The idea is to read data directly from Google Docs. And it allows this:To be turned into something like this:Or into an HTML page, or into .docx file, or .epub, or basically into anything Pandoc supports. So let’s go through step by step how this works.The headerHeader is a simple block of YAML. We are telling it the date, author, title, the location of a bibliography and other such details. It depends a bit from our desired output format what we want to have here, as we can also pass many of these settings directly into Pandoc. Here we just save the header as a character string.header &lt;- '---title: "Collaborative authoring in Google Docs"author: "Niko Partanen"date: "19 Jun 2015"pdf_document:        keep_tex: yesbibliography: ./bibtex/FRibliography_example.bib---'Reading the fileThen we read the file from Google Docs and write it as a file. The link to Google Docs is very easy to generate. You can get the document ID from the link you use to share it.The structure is like this:https://docs.google.com/document/d/put-id-here/export?format=txtOne could have there also some other format, but a simple text file is the best for now.library(readr)doc &lt;- read_file("https://docs.google.com/document/d/1D1C_79nLDM25r96dWinufVUd70-ipsHzv6cK-9cK-Io/export?format=txt")fileConn &lt;- file("temp.txt")writeLines(doc, fileConn)close(fileConn)Removing the NUL characterFor some reason the text file contains a NUL character in the beginning of the first line. I don’t understand at all what it is doing there, but R, LaTeX and Pandoc are all giving errors for it later, so having it around is unacceptable. I was trying to find out how to delete it for quite some time, and ended up to do it from Terminal, by simply removing those first bytes.system("cat temp.txt | tail -c +4 &gt; temp_clean.txt")doc &lt;- read_file("temp_clean.txt")I would be very happy to know if there is some easier way to do this!Removing the commentsIt is possible that there are some comments left into the document. They have formatting like [a] or [b] in the text, and the actual comment text comes to the end. I assume there could be some way to turn them into footnotes in the PDF, but at the moment I just want to delete them.doc &lt;- gsub("(.+## References).+", "\\1", doc)doc &lt;- gsub("\\[.\\]", "", doc)Putting it togetherThen we simply paste together the header and the document. This is then written into a new Markdown document.full_paper &lt;- paste0(header, doc)fileConn &lt;- file("example.md")writeLines(full_paper, fileConn)close(fileConn)Converting the filesAfter this we can start to convert the file. Now I convert it into HTML and PDF. Note for linguists: The default fonts used here are not very good for any exotic script. There are lots of situations where the basic fonts in principle can handle what you have there, but in some specific situations (such as within a code block) the font in that environment doesn’t have what you need. Well, this is how the things always are with typography and lesser used languages. But remember to play with Pandoc font settings if you have any non-latin stuff there.library(rmarkdown)render('example.md', output_format = "html_document")system("pandoc example.md --latex-engine=xelatex --biblio ./bibtex/FRibliography_example.bib -o example.pdf --variable mainfont=Georgia")We can even turn it into an EPUB! Not that this would be necessary, but it is possible! I like epubs a lot, not really for scientific texts, but for normal books they are pleasant to use.system("pandoc example.md --biblio ./bibtex/FRibliography_example.bib -o example.epub")Now there are already some font issues rising their ugly heads…As the last thing I also convert the PDF into a picture used in the beginning of this post. It uses free program ImageMagick.system("convert -density 300  example.pdf example.png")The script can be downloaded in one piece from here."
    } ,
  
    {
      "title"    : "Concatenating MTS files",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/05/21/converting-mts-files.html",
      "date"     : "2015-05-21 01:55:14 +0200",
      "content"  : "We are having a good recording session in the village of Krasnoye, Nenetskiy Avtonomnyj Okrug, Northern Russia, when we realise that our new good video camera is filling up the sd-card much faster than we anticipated. Everything on the card is backed up to several external hard drives, I have one copy at my laptop even - let’s make there some space. It is not so good practice to touch your SD-cards, they are so cheap that they never really should need to be reused, but this situation demanded radical solutions. I go and remove some of the backupped .MTS files - that is the actual video file, right?Hell broke loose.The consequences were not evident very soon. However, the AVCHD file format is a bit more than the actual video files. I had naively thought that the files in the other folders must be for cameras internal work, thumbnails, ok, some playlist, I guess the camera needs those!When I started to edit the videos I realized this folder structure indeed kept some more information which was crucial for us. When the camera shoots the video, it automatically cuts the video into smaller pieces when they are large enough (with our settings that is something after half an hour, around 2,12 gigabytes per file). The way how these two files exactly combine, as far as I understand it now, is in .CPI files in CLIPINF folder.ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:30.0 -c copy kpv_izva20150000-1-cut.MTSNow the synchronization is succesful, the approach works, but we are clearly removing too much. It is a matter of just a few frames. The removal should not be detectable in the final result.We know that the video frame rate is 25 fps. Then one frame should be 0.04 seconds. After some testing I found that removing 16 first frames gives us a clean join. This doesn’t happen with any less frames being removed:ffmpeg -i kpv_izva20150000-1.MTS -ss 00:00:00.68 -c copy kpv_izva20150000-1-cut.MTSDoing this processing takes around half a minute, so it is fast enough to test this multiple times with different configurations.However, when we open the file in Final Cut Pro X there is still something wrong. An annoying small green flash. However, it seems to be the case that we are somehow hitting the right spot with this cut, as any attempt to cut more seems to result in FCPX inserting black timeline between the clips. Now the clips join smoothly, which must be how it is supposed to be. Now we are not losing anything from the audio track which is within those two videos. That small green flash is as long as three frames, that makes 0.12 seconds. What I ended up doing was to repeat the previous frame three times, in a configuration more or less like this:IMAGE HERE!This leaves to video tiny tiny portion where data is missing, but in the end of the day it is still manageable loss, at least not the worst that could happen with data mismanagement.QuestionsWhat are we supposed to do with AVCHD files? That is the original raw data we get. Now it seems that the easiest way to access that data is through a commercial software such as Final Cut Pro X. This is fine, as we anyway have to use something like that to edit the files. However, is the resulting MOV file exactly the same as the original files in what comes to quality? What exactly happens with those files in AVCHD structure when they are opened in FCPX? There are some other open questions, mainly with archiving.      Should we archive the AVCHD files? If so, how they fit into our session structure? They are usually 30 GB files with lots of session in one large file. They also contain different binary files, would those be accepted as well?        How about archiving the session related Plural Eyes Project files (.pep) and Final Cut Pro XML (.fcpxml)? Those two files contain all information about the process how the raw files have been transformed into sessions ready to be transcribed.  "
    } ,
  
    {
      "title"    : "Moving files in Terminal",
      "category" : "",
      "tags"     : "Terminal",
      "url"      : "/2015/05/19/moving-files.html",
      "date"     : "2015-05-19 01:55:14 +0200",
      "content"  : "It is very common that we have a large amount of files in one folder, and then for whatever reason we decide that we would need to create for each filename a unique folder, where the different files sharing same name but different endings could reside. I had recently lots of ELAN files which I wanted to move into their own directories. I believe this is a very common situation, so I wanted to share it.  It can be done with these three commands. Just copy paste them one by one to the Terminal.First command: This makes the directories with the name of each ELAN filels | grep '\.eaf$' | sed 's/.eaf//g' | xargs mkdirSecond command: This uses grep to pick all ELAN files, then creates variables for different things and repeats the move for each file#!/bin/bashfor file in `ls | grep '\.eaf$'`do    SESSIONNAME=$(echo $file | sed 's/.eaf//g')    MOVFILE="$file"    MOVDESTINATION="./$SESSIONNAME/$file"    /bin/mv $MOVFILE $MOVDESTINATION;doneThird command: This does exactly the same as the second but for pfsx files.#!/bin/bashfor file in `ls | grep '\.pfsx$'`do    SESSIONNAME=$(echo $file | sed 's/.pfsx//g')    MOVFILE="$file"    MOVDESTINATION="./$SESSIONNAME/$file"    /bin/mv $MOVFILE $MOVDESTINATION;doneI did the same recently with a folder full of mp3 and wav files. The model is identical, just the filename differs. This can be a convenient way to rearrange messy file structure, just to start dragging everything into one folder and move onward with commands like these. This also forces to check that all files are consistently named, which is of course necessary for well structured corpus."
    } ,
  
    {
      "title"    : "Spectrograms with Spek",
      "category" : "",
      "tags"     : "audio, recording",
      "url"      : "/2015/05/19/spectrograms-with-spek.html",
      "date"     : "2015-05-19 01:55:14 +0200",
      "content"  : "One of the problems in ELAN is that it doesn’t really give you a good picture from the wave form for the whole file. Often this is full of visual cues that help us to find relevant pieces, i.e. songs, where new section starts etc. Naturally for different purposes we need different views, and this means use of different programs which handle some aspect particularly well.To get overview into the file, I’d say that among free programs Audacity does this well, Reaper is another good low-cost option, but I think the way Audacity produces the view is the most telling. (I bet one can produce waveforms also directly from R with Seewave R package, but this is an experiment for another time.)One thing we would need every now and then is to have very clean spectrograms from the audio in order to see what is really going on at the different frequencies, especially the highest ones. This tells us a lot about the background noices of the recording space.This example is from one session which was done indoors in a quiet environment, closed door, closed windows. But there was some periodic work going on outside which we didn’t really hear through closed window, but it was there. Some sort of machinery operating pretty far away from us, but still apparently close enough for microphones to catch something from it.The pattern here reminds the one produced by every fieldworkers enemy number one: the Russian refridgerator. This is something less problematic, not really even audible in itself. Ideally we would have view like this into our recordings while they are being done.This also reminds from Nyquist–Shannon sampling theorem. It states that the highest frequency that can be captured is approximately half from the sampling rate chosen. As our sampling rate is always 48 kHz, it means that we would capture something up to 20+ kHz. This seems to be the case. And it is of course reasonable as we capture basically the range a human can hear and a bit extra.What would happen if the sampling rate would be higher? As a very young man I was doing fieldwork in Karelia, and indeed some time had set the sampling rate into 96 kHz. I must have been thinking that if I have the settings high enough at least I can’t screw it up! Well, I guess there is no harm done, only wasted hard disk space.Suddenly there are higher frequencies! And this must be a consequence of higher sampling rate. These both files were recorded with Edirol HR-09 recorder, but the first one with an external lapel mic. That explains the exceptional clarity of the signal.In principle, if one would record in a cave full of bats, then would their high-frequency noise appear as distortion on 48 kHz recording but be captured adequately at the 96 kHz recording? The problem is that those high frequency sounds are present in the lower sampling rate too, they just are not there so systematically that their actual form would had been captured! Something to think about (yet nothing to act upon, this is a theoretical question)!Things to do  Check what is going on at the Praat level of view with higher frequency noises on and off  Simulate the environment with lots of above-hearing-level frequency noise and see what happens to the recordings"
    } ,
  
    {
      "title"    : "Welcome to the blog",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/05/17/langdoc.html",
      "date"     : "2015-05-17 01:55:14 +0200",
      "content"  : "While we document the endangered languages of the world there is quite a lot to do. Data quality, data content, connection with the speech communities, managing different audio and video files – this is in many ways extremely difficult job to do. One would assume that the technical solutions which we use would had been designed to make this work as easy as possible. Somehow this does not seem to be the case. Technology is just a tool, our real questions have nothing to do with it. This blog functions mainly as a platform where one can collect notes and tips about how to do different things.How often do we update our archived data? I think it is often done maybe every half a year, just because the uploading process is so inconvenient, slow and unreliable. I’m certain this is not intentional. Surely everyone has done their part well and the tools are designed to work in the best possible manner. But something has gone wrong.Filling metadata should be so easy that one does it just for joy of keeping things ordered the easiest possible way. Similarly, archiving should be so easy that one just syncs the newest files in the end of the day. Press a button, collect your stuff, tidy your desk – beep – sync done. Repeat every day you do something with your data.This should be possible with current technology. We believe this doesn’t need to be so difficult. There are better solutions for almost any aspect of our digital data management, and we are going to figure out what those are.At times the tools we need do not exist. But then we have to build them. Luckily, programming nowadays has started to remind more and more building with Legos. There are so many packages and modules for all kinds of languages (R, Python, Ruby – whatever one needs) floating around GitHub and other repositories that often 70% of the work is already done. It’s just about combining those pieces together in a way that contributes to language documentation workflow.This site, LangDoc, has been built in order to disseminate our ideas and practices. It is divided into individual blogposts. We also plan having independent daughter sites for specific projects and datasets."
    } 
  
  ,
  
   {
     
        "title"    : "About",
        "category" : "",
        "tags"     : "",
        "url"      : "/about/",
        "date"     : "",
        "content"  : "You can find the source code of this page from github.com/langdoc. Our goal is that LangDoc site would gather different modern solutions to language documentation, and help to disseminate the best practices.Idea is that the tools for different workflows would be directly accessible in their own GitHub repositories. This also allows more collaborative work and discussion around them, as an example, in the form of GitHub issues.Please contact us if you have any questions. The contact information is below.LicenceCC-BY"
     
   } ,
  
   {
     
        "title"    : "Categories",
        "category" : "",
        "tags"     : "",
        "url"      : "/categories.html",
        "date"     : "",
        "content"  : "Categoriesaudio                                                  Spectrograms with Spek                      recording                                                  Spectrograms with Spek                      Terminal                                                  Adding shadows to images                          Moving files in Terminal                      R                                                  Generating sine-waves with R                          Fieldwork map                          Google Docs and Markdown                      workflows                                                  Google Docs and Markdown                      fieldwork                                                  Data workflow for linguistic fieldwork                          Fieldwork map                      emacs                                                  Editing text blocks in Emacs                      digitalization                                                  Editing text blocks in Emacs                      data                                                  Generating sine-waves with R                          Data workflow for linguistic fieldwork                      maps                                                  Adding shadows to images                      "
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
   } ,
  
   {
     
        "title"    : "Search",
        "category" : "",
        "tags"     : "",
        "url"      : "/search/",
        "date"     : "",
        "content"  : ""
     
   } ,
  
   {
     
   } ,
  
   {
     
   } 
  
]
