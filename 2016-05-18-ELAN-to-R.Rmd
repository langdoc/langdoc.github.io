---
title: "Reading ELAN files to R"
author: "Niko Partanen"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
excerpt_separator: <!--more-->
bibliography: ~/FRibliography/bibtex/FRibliography.bib
---

```{r, echo=F}
library(plyr)
suppressPackageStartupMessages(library(dplyr))
library(FRelan)
```


### What this is about

This treatment may appear little bit chaotic and unstructured, but this is only because the topic in hand is relatively complicated. I try to explain it to different audiences, but I also understand that following the whole text demands knowledge about some documentary linguistic practices and programming language R. The main idea is to present ways to use R to work with ELAN files. The main idea is to get from something like this:

![Elan file](media/elan1.png)

Into something like this:

![Elan file and some metadata in R data frame](media/images/elan-R.png)

As one can see, not all data in this data frame comes from ELAN file directly. In my opinion this is one of the major reasons to use R to work with ELAN files. Associating session name or speaker id with any arbitrary pieces of information suddenly become trivially easy. In this case I have just merged some metadata with the tokens, but naturally there is no end to what one can have there. I have sorted the data here by speaker and the time, whereas sorting it only with time would of course bring to the table the natural order in which the utterances were spoken. In principle one could use more complicated data structures to store the ELAN file's content, but in my opinion a data frame is just fine since it is very easy to work with. I will go through now only how one reads ELAN files -- metadata comes later. I can just say briefly that what I'm usually doing is to open database connection from R to FileMaker Pro and to read the metadata from there. Now perfect, but works fine and is easy.

I wrote already some year ago a small R package that can be used to parse ELAN files. We've called it `FRelan`, as it is developed with Freiburg based language documentation traditions. There is just one small problem -- it works out of the box only with our ELAN files. The reason is that ELAN files can be structured in almost any possible way. As far as I see it, it is almost impossible to write a package which would just work automatically. However, I think it is possible to write some kind of generic tools which can be adapted into most of the files, and following the same logic one can also parse more exotic tier structures if the need arises.

FRelan package can be installed from GitHub with `devtools` package:

    install.packages("devtools")
    devtools::install_github("langdoc/FRelan")

However, one should probably read this post thoroughly before attempting to use it. (And do not try to do that before this post is 'officially' published, because it is guaranteed to be work very weirdly!)

In an ideal world one could read ELAN file into R just typing:

    FRelan::read_eaf(file = "elan_file.eaf")

And this is how it works with our ELAN files, but I want to help others to achieve the same with their corpora!
### Introduction

This writing is part of the wider topic of how to get documentary linguistic data into R. This means not only reading in ELAN files, but doing the same for metadata and other accompanying datasets we may have with our corpus. There are many reasons one may want to have documentary linguistic corpus in R. It makes searching the data very easy, and one is not limited to the predefined possibilities of individual user-interface. Besides this it is easy to make the study entirely reproducible, as there is no unnecessary mouse clicking involved. R has already for many years been used in variationistic sociolinguistics, and also in that realm use of ELAN has been around [@nagyEtAl2015a]. In their present paper Nagy and Meyerhoff suggest a possible workflow. However, this workflow is also based upon exporting the ELAN files into CSV format, which is then being read into R. Reading these files into R is trivially easy, but it creates uneasy situation where doing changes in the data demands creation of a new export file. The situation gets even more problematic if there are further annotations done to those spreadsheets, as it can get quite messy to append the changes from modified ELAN files into annotated tables. This is very typical problem, in my opinion, and there are very few satisfactory solutions to the general problem (which also persists in ELAN when the transcriptions are modified but there are annotations already on the tiers below).

All treatments of data analysis with R start with the notion that one has to get the data into R (see, for example [@wickhamEtAl2016a]). Getting data into R is basically a data transformation, and the method of doing the import is to define this transformation and run it whenever needed. So the question can also be turned into a statement: if data can be transformed, then it can be read into R. By transformation I mean any kind of operation where the data is parsed, in its totality or with some subset, and turned into whatever other data structure we need without information being lost. Or if the information is lost, it should be clearly understood and acknowledged.

In order to be able to make this kind of transformations to the corpus, the data structures have to be perfectly described and systematically present in the files. Actually making sure that this is the case would be very easy, for example, with [parameterized RMarkdown reports](http://rmarkdown.rstudio.com/developer_parameterized_reports.html). The R functions to parse ELAN files are kind of backbones of this kind of testing, about which I should probably be writing more soon.

I think often when there is talk about data formats there is some confusion over the fact that the data should be in different structures in different parts of its life. Of course these stages are not really distinct from one another, but there are clear conceptual differences and requirements which I try to outline below. 

- data editing: minimal repetition of fields
- data archiving: maximal usability for different purposes
- data analysis: wide applicability for different statistics and plotting

All these stages have different demands for the ideal data structures, and we should not imagine that there is just one solution for this. Naturally this is a two way street, so that during data analysis one often needs to edit the data and come back to analysis. If we think about the resulting dataset in terms of **tidy data**, where we would ideally have one row per one observation [@wickham2014a], then I think we often should consider **token** as the basic unit here. The resulting data is relatively ineffective in the sense that the same information is repeated all over the place, but it is **tidy** according to some specifications. It can be described also as **long**, as there are relatively few columns and the data is stored along the rows. This kind of data structure is not supposed to be edited by human, but computers crunch it very happily.

### Technical part

There are many ways to parse ELAN file into R, and ultimately it boils down to the question **what do you want to do**? Usually I want to search from the data in R or I want to merge some content with metadata, either to make more complex searches or to check what is going on. However, the scope varies. If I think in terms of ELAN's tier structure, there are following attributes we can use:

- linguistic type (ideally each file would have one tier of one type per speaker)
- speaker
- tier name (although this is complicated -- what is tier name but combination of attributes above?)

Reading ELAN files into R is not trivially easy and it gets fast very complicated. The main question is always how uniform the ELAN files are in their basic structure. This is really where all problems begin. If all ELAN files are identical, then one can just specify the pattern to parse one and loop it over all files. However, in reality the situation is much more complicated, and ELAN files tend to be somewhat heterogenic. There is nothing in ELAN that would prevent small creeping in here and there, and what may look harmless for a human, for example, slightly different name of a linguistic type, is of course impassable barrier for a computer which tries to read the file. So the code to parse ELAN files often needs to have all kinds of bells and whistles attached into it to handle errors, which makes the code very complicated and there is always possibility that something fails in a new way.

However, this is what makes parsing ELAN files to R particularly rewarding as well. It tells where the problems are, and makes it easier to address them. It can thereby be tested also as a robust testing method which helps to keep ELAN files organized. As far as I see it, the fact that all files adhere to uniform structure should be among the most basic definitions we have for **corpus**, differentiating it from eclectic file collections. This comes back to what I was describing above: if corpus is parsable, it is also transformable, and this is actually guarantees its longevity. It is not likely that in thirty years we want data to be in exactly this kind of XML files, but if they are perfectly uniform doing the transformation will never be more than a fast and easy task, whereas currently that can be quite a nightmare.

#### Testing if tier exists

I'm always ranting about the lack of uniformity in corpora, so let's come up first with some tools we can use to make the actual parsing of ELAN files more reliable. First of all we need something to check whether a tier exists. This is already very useful, for example, while looping across the files and trying to sniff the files which are missing some crucial tiers. Or maybe we want to get rid of some tiers! Whatever the goal, this helps to hunt them down.

```{r}
tier_exists <- function(eaf_file, tier){
        
        match <- xml2::read_xml(eaf_file) %>% xml2::xml_find_all(xpath = paste0("//TIER[@TIER_ID='", tier, "']"))
        if (length(match) >= 1){
                TRUE
        } else {
                FALSE
        }
}

tier_exists(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "word@JAI-M-1939")
tier_exists(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "token@JAI-M-1939")

```

#### Testing if tier is empty

This little function does two things -- and can be used uncountable ways! If the value is specified as `TRUE`, it gives the number of items on that tier. This is often useful to know! However, it can also just tell us whether the tier is empty or not. For example, one could easily loop this across the files to find out all ELAN files where the tier that is supposed to contain the tokenized words has not been populated by the tokens.

```{r}

tier_has_content <- function(eaf_file, tier, value = F){
        
        if (tier_exists(eaf_file = eaf_file, tier = tier) == F){
        print("Tier doesn't exist!")
                } else {
        tier_content <- xml2::read_xml(eaf_file) %>% 
                xml2::xml_find_all(xpath = paste0("//TIER[@TIER_ID='", tier, "']/ANNOTATION/*/ANNOTATION_VALUE")) %>%
                xml2::xml_text()
        }
        
        if (value == FALSE && tier_exists(eaf_file = eaf_file, tier = tier) == T) {
                result <- length(tier_content) > 0
                print(result)
        }
        
        if (value == T && tier_exists(eaf_file = eaf_file, tier = tier) == T) {
        length(tier_content)
        }
}

# This tier has content
tier_has_content(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "word@JAI-M-1939", value = F)
# This tier exists, but has no content
tier_has_content(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "phonet-UPA@JAI-M-1939", value = F)
# This tier is full of content, actually
tier_has_content(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "word@JAI-M-1939", value = T)
# This tier doesn't exist
tier_has_content(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", tier = "token@JAI-M-1939", value = T)

```


#### Reading one tier

I go first through how to set up a function to read one tier, since in principle the rest is combining this in different ways. In principle one could take this function and combine it in smart ways to read the complete file.

```{r, eval=F}

read_tier <- function(eaf_file = "/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf", participant = "JAI-M-1939", linguistic_type = "wordT", independent = F){

        `%>%` <- dplyr::`%>%`

        file <- xml2::read_xml(eaf_file)
        
        create_path <- function(..., above = F){
                if (exists("participant")){
                        restriction <- paste0("//TIER[@LINGUISTIC_TYPE_REF='", linguistic_type, "' and @PARTICIPANT='", participant,"']")
                } else {
                        restiction <- paste0("//TIER[@LINGUISTIC_TYPE_REF='", linguistic_type, "']")
                }
                if (above == T){
                        xpath_end <- "/ANNOTATION/*/ANNOTATION_VALUE/../../.."
                } else {
                        xpath_end <- "/ANNOTATION/*/ANNOTATION_VALUE/.."
                }
                
                file %>% xml2::xml_find_all(paste0(restiction, xpath_end))
        }

        dplyr::data_frame(
                Content = create_path() %>% xml2::xml_text(),
                annot_id = create_path() %>% xml2::xml_attr("ANNOTATION_ID"),
                ref_id = create_path() %>% xml2::xml_attr("ANNOTATION_REF"),
                participant = create_path(above = T) %>% xml2::xml_attr("PARTICIPANT"),
                tier_id = create_path(above = T) %>% xml2::xml_attr("TIER_ID"),
                type = create_path(above = T) %>% xml2::xml_attr("LINGUISTIC_TYPE_REF"))
        }

```


#### Reading whole ELAN file

What there is in ELAN file can be thought in following terms:

- tiers which have identical basic structure
    - with the difference that some have references to the time codes
- in principle all tiers have time references, but those are accessible from their parent tiers (or their parents)
    - I mean that even with annotations on tiers like **symbolic subdivision** it is possible to locate the span during which they are located on recording (time span of their parent tier)

```{r note_core, eval=F, echo=F}
        #        file %>% xml2::xml_find_all(paste0("//TIER[@LINGUISTIC_TYPE_REF='", linguistic_type, "']")) %>%
        #                xml2::xml_attr("PARTICIPANT") -> participant
```


### References
